{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c0bde21abcdc4adb99227af9741072fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_593b2603dd5248ca88b395beac5e837a",
              "IPY_MODEL_547d0d7934d945c0a1167af422fd6e0e",
              "IPY_MODEL_e11d0767a3414fd692d4bd928f661667"
            ],
            "layout": "IPY_MODEL_66f9b1da7fe34dc1923e9f40a6b7850a"
          }
        },
        "593b2603dd5248ca88b395beac5e837a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bd279957815f4e53b6f8c099070df063",
            "placeholder": "​",
            "style": "IPY_MODEL_0b90d74c69104d888a87c6765e915025",
            "value": "Downloading (…)solve/main/vocab.txt: 100%"
          }
        },
        "547d0d7934d945c0a1167af422fd6e0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fa752624ae8a4e46b6ce8620dea2cc34",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8623a3d7c48d40238fec15b719d32db7",
            "value": 231508
          }
        },
        "e11d0767a3414fd692d4bd928f661667": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f335176d8c8e4c9d846da08d0eaadf15",
            "placeholder": "​",
            "style": "IPY_MODEL_8b94dea30c1c4d52b70f699f3da6e85a",
            "value": " 232k/232k [00:00&lt;00:00, 2.60MB/s]"
          }
        },
        "66f9b1da7fe34dc1923e9f40a6b7850a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bd279957815f4e53b6f8c099070df063": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b90d74c69104d888a87c6765e915025": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fa752624ae8a4e46b6ce8620dea2cc34": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8623a3d7c48d40238fec15b719d32db7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f335176d8c8e4c9d846da08d0eaadf15": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b94dea30c1c4d52b70f699f3da6e85a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7bf1fe11f9ad4221a108e66b82a758e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_01ba0a86c66541f8919bf26c39fb22a4",
              "IPY_MODEL_5ca99313a9f6474d999252b4d62bcc57",
              "IPY_MODEL_fb3bd3b936054be18e1206e4d6c61c6e"
            ],
            "layout": "IPY_MODEL_d7bd152f3b8740b1aa0edf607c0c2547"
          }
        },
        "01ba0a86c66541f8919bf26c39fb22a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_699c9c099d1f41f29f50f20ebfe29d36",
            "placeholder": "​",
            "style": "IPY_MODEL_c5b9fc02b5724583a54a3b67248defeb",
            "value": "Downloading (…)okenizer_config.json: 100%"
          }
        },
        "5ca99313a9f6474d999252b4d62bcc57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7975baa9c0f547e3bd7e8b542b8e79f0",
            "max": 28,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a19dfab5736646888afdbaa78996ac0f",
            "value": 28
          }
        },
        "fb3bd3b936054be18e1206e4d6c61c6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e808a0ffaba04d90be253c34b0732943",
            "placeholder": "​",
            "style": "IPY_MODEL_782f9887ef5046afaebf34d1f310466e",
            "value": " 28.0/28.0 [00:00&lt;00:00, 579B/s]"
          }
        },
        "d7bd152f3b8740b1aa0edf607c0c2547": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "699c9c099d1f41f29f50f20ebfe29d36": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5b9fc02b5724583a54a3b67248defeb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7975baa9c0f547e3bd7e8b542b8e79f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a19dfab5736646888afdbaa78996ac0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e808a0ffaba04d90be253c34b0732943": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "782f9887ef5046afaebf34d1f310466e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7ae1f57eb1644050a614b751ae75b40b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ff0a1a5d1c0d49229194d9ae17d942b3",
              "IPY_MODEL_ce648a1430164f5b8ea7a0f120762e9f",
              "IPY_MODEL_b0a975f9c3324ee58a5fb0f49c952149"
            ],
            "layout": "IPY_MODEL_25e364323fda4ba39a3c3294db2ec7a7"
          }
        },
        "ff0a1a5d1c0d49229194d9ae17d942b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b0e22b5472b5439eb505adbf66d2bda5",
            "placeholder": "​",
            "style": "IPY_MODEL_43b5992cb5d64439abd4583c1eec832d",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "ce648a1430164f5b8ea7a0f120762e9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_021006f74dce465eb3e9fc9382acc6e1",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_79a545075bca41c097224c94cefcac2b",
            "value": 570
          }
        },
        "b0a975f9c3324ee58a5fb0f49c952149": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4a1757a7e520402f86cdcb5d0ec3df1f",
            "placeholder": "​",
            "style": "IPY_MODEL_86850656e51346669ae534a5729ccadf",
            "value": " 570/570 [00:00&lt;00:00, 27.3kB/s]"
          }
        },
        "25e364323fda4ba39a3c3294db2ec7a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b0e22b5472b5439eb505adbf66d2bda5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43b5992cb5d64439abd4583c1eec832d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "021006f74dce465eb3e9fc9382acc6e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "79a545075bca41c097224c94cefcac2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4a1757a7e520402f86cdcb5d0ec3df1f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "86850656e51346669ae534a5729ccadf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f31464ac755c4387b9a0e1d7c1f6de0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d26b06d195434ac08f04a0a1123e0a70",
              "IPY_MODEL_ca64c90adc3d45e190098de6386312a9",
              "IPY_MODEL_d9e4a71f8f0c4e29a828d8ecfbebdbf1"
            ],
            "layout": "IPY_MODEL_76c7f22db9474a42865b98ee4bfa3f66"
          }
        },
        "d26b06d195434ac08f04a0a1123e0a70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_648a12665c9242359d623f692564c267",
            "placeholder": "​",
            "style": "IPY_MODEL_d4bf5deb520449ff9f9eeb4d596f1f9c",
            "value": "Downloading (…)&quot;pytorch_model.bin&quot;;: 100%"
          }
        },
        "ca64c90adc3d45e190098de6386312a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_17d68d2d762d40149a5d0d26bb966d59",
            "max": 440473133,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_76ff9e6510ea43cfb350a3d0604fbcb8",
            "value": 440473133
          }
        },
        "d9e4a71f8f0c4e29a828d8ecfbebdbf1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_62ee0c47991b48d68f44852da1843f8f",
            "placeholder": "​",
            "style": "IPY_MODEL_83e66e83add24508a0ee09ad23e052a7",
            "value": " 440M/440M [00:04&lt;00:00, 128MB/s]"
          }
        },
        "76c7f22db9474a42865b98ee4bfa3f66": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "648a12665c9242359d623f692564c267": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4bf5deb520449ff9f9eeb4d596f1f9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "17d68d2d762d40149a5d0d26bb966d59": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "76ff9e6510ea43cfb350a3d0604fbcb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "62ee0c47991b48d68f44852da1843f8f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "83e66e83add24508a0ee09ad23e052a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "paf9eh4HS6Oc",
        "outputId": "c80a3f4f-072b-42dd-bed5-f7cfddd7b69d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import gensim\n",
        "import string\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('stopwords')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = nltk.corpus.stopwords.words('russian')\n",
        "def preprocess(doc):\n",
        "    # к нижнему регистру\n",
        "    doc = doc.lower()\n",
        "    # убираем пунктуацию, пробелы, прочее\n",
        "    for p in string.punctuation + string.whitespace + 'http': \n",
        "        doc = doc.replace(p, ' ')\n",
        "    # убираем кавычки\n",
        "    for p in ['«', '»', '\\'', '\\\"']:\n",
        "        doc = doc.replace(p, ' ')\n",
        "    for p in stop_words:\n",
        "        doc = doc.replace(' ' + p + ' ', ' ')\n",
        "    # убираем лишние пробелы, объединяем обратно\n",
        "    words = doc.split()\n",
        "\n",
        "\n",
        "   \n",
        "    \n",
        "    return words"
      ],
      "metadata": {
        "id": "0va7TV_nTTS7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/news.csv')\n",
        "df = df[df['rubric'].notna()]"
      ],
      "metadata": {
        "id": "V5oyPq--Tbyc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop(['source','title','publication_date','subrubric','tags'],axis =1 )"
      ],
      "metadata": {
        "id": "-kUiONRKAm4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "ns0zH3M2BO7Z",
        "outputId": "f1e28ccb-7c87-4b42-f593-559456c1d95c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                   text          rubric\n",
              "0     В 1930-е годы Советский Союз охватила лихорадк...       Экономика\n",
              "1     Олимпийская чемпионка по фигурному катанию  Ал...           Спорт\n",
              "2     Российский врач-диетолог Римма Мойсенко объясн...        Из жизни\n",
              "3     В 2019 году телеканал «Ю» запустил адаптацию з...  Интернет и СМИ\n",
              "4     Актер  Михаил Ефремов  систематически употребл...        Культура\n",
              "...                                                 ...             ...\n",
              "4505  C 1 января Болгария начала получать российский...       Экономика\n",
              "4506  В России вступили в силу новые правила содержа...          Россия\n",
              "4507  Российский певец  Сергей Лазарев  объяснил сво...        Культура\n",
              "4508  Президент США  Дональд Трамп  заявил о нежелан...             Мир\n",
              "4509  В состав  Совета Безопасности ООН  вошли пять ...             Мир\n",
              "\n",
              "[4503 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9c0d43b1-a23c-4ca3-892e-2c716c1f73c0\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>rubric</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>В 1930-е годы Советский Союз охватила лихорадк...</td>\n",
              "      <td>Экономика</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Олимпийская чемпионка по фигурному катанию  Ал...</td>\n",
              "      <td>Спорт</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Российский врач-диетолог Римма Мойсенко объясн...</td>\n",
              "      <td>Из жизни</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>В 2019 году телеканал «Ю» запустил адаптацию з...</td>\n",
              "      <td>Интернет и СМИ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Актер  Михаил Ефремов  систематически употребл...</td>\n",
              "      <td>Культура</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4505</th>\n",
              "      <td>C 1 января Болгария начала получать российский...</td>\n",
              "      <td>Экономика</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4506</th>\n",
              "      <td>В России вступили в силу новые правила содержа...</td>\n",
              "      <td>Россия</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4507</th>\n",
              "      <td>Российский певец  Сергей Лазарев  объяснил сво...</td>\n",
              "      <td>Культура</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4508</th>\n",
              "      <td>Президент США  Дональд Трамп  заявил о нежелан...</td>\n",
              "      <td>Мир</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4509</th>\n",
              "      <td>В состав  Совета Безопасности ООН  вошли пять ...</td>\n",
              "      <td>Мир</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4503 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9c0d43b1-a23c-4ca3-892e-2c716c1f73c0')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9c0d43b1-a23c-4ca3-892e-2c716c1f73c0 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9c0d43b1-a23c-4ca3-892e-2c716c1f73c0');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['rubric'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNK3C9LTBRn2",
        "outputId": "3f59caf2-8884-4210-f0cc-e48b7e73cc55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Россия               908\n",
              "Мир                  718\n",
              "Спорт                494\n",
              "Экономика            420\n",
              "Бывший СССР          394\n",
              "Силовые структуры    220\n",
              "Интернет и СМИ       211\n",
              "Наука и техника      208\n",
              "Культура             206\n",
              "Из жизни             197\n",
              "Путешествия          183\n",
              "Ценности             141\n",
              "Дом                  123\n",
              "Нацпроекты            53\n",
              "69-я параллель        27\n",
              "Name: rubric, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Выбрал 2 рубрики для бинарной классификации \n"
      ],
      "metadata": {
        "id": "dpoDEJnABvzk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['text'] = df['text'].map(preprocess)\n",
        "\n",
        "docs = list(df['text'].values)"
      ],
      "metadata": {
        "id": "FGiVvuvKUAPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2v = gensim.models.Word2Vec(sentences = docs, # список из списков, в каждом str\n",
        "                             size = 300, # размер пространства признаков\n",
        "                             window = 5, # длина двигающегося окна\n",
        "                             min_count=5, # минимальное кол-во слов в окне\n",
        "                              \n",
        "                             sg=1, # используем skip-gramm\n",
        "                             iter=50) # кол-во итераций"
      ],
      "metadata": {
        "id": "f77NL94wUEP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.loc[(df['rubric'] == 'Мир') | (df['rubric'] == 'Россия')]\n",
        "#Оставил только 2 рубрики для бинарной классификации"
      ],
      "metadata": {
        "id": "sJON2voXGpgN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector = w2v.wv.get_vector('россия')"
      ],
      "metadata": {
        "id": "0WFUuoLW2A4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p23dHJDsCfIh",
        "outputId": "40a194b4-9ee8-486c-84bf-8763cfafd233"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-1.61289588e-01, -5.45829535e-01, -2.20198095e-01,  3.63287300e-01,\n",
              "       -1.30189583e-01,  7.44966149e-01,  1.84165448e-01,  5.72970882e-02,\n",
              "        7.79301375e-02,  3.04318428e-01, -5.69819689e-01,  5.54751456e-01,\n",
              "       -1.87646344e-01,  2.80744761e-01, -5.53612530e-01,  9.57258940e-02,\n",
              "       -3.11739207e-03,  1.64042979e-01,  2.22425815e-02, -4.18082863e-01,\n",
              "       -4.47781891e-01,  3.10273975e-01, -4.49311823e-01,  1.35721520e-01,\n",
              "        1.87423483e-01,  2.11014867e-01,  1.06136963e-01, -4.37411636e-01,\n",
              "       -2.96020031e-01, -1.54076114e-01,  6.95673704e-01, -2.36655891e-01,\n",
              "        4.27475244e-01,  3.73083681e-01, -4.82880265e-01,  7.79307067e-01,\n",
              "       -5.65028906e-01,  2.03841731e-01,  1.24685131e-01,  2.29135320e-01,\n",
              "        2.14554116e-01, -7.18565956e-02,  6.64239585e-01, -3.53487849e-01,\n",
              "        1.04368618e-02,  5.19015431e-01, -6.53775513e-01, -3.12402844e-01,\n",
              "        1.61327705e-01, -1.90802440e-01,  1.18759302e-02, -1.97168346e-02,\n",
              "       -7.41018057e-01,  2.60159433e-01, -4.42006975e-01,  1.17878675e+00,\n",
              "        4.22526337e-02, -1.61902413e-01, -4.49456304e-01, -2.13760883e-01,\n",
              "       -1.28811970e-02,  2.51861393e-01, -2.94127285e-01,  4.53667760e-01,\n",
              "        1.86372199e-03,  8.04552659e-02,  2.72920042e-01,  3.43196750e-01,\n",
              "       -1.26625717e-01, -1.00338471e+00,  4.46351856e-01,  1.37776583e-01,\n",
              "        4.23130244e-01,  3.64185959e-01,  2.29697451e-01,  2.13829175e-01,\n",
              "        1.69011816e-01,  2.77877539e-01, -6.68670386e-02, -1.60933122e-01,\n",
              "        3.45858812e-01, -2.99664408e-01,  2.14370131e-01, -4.22191769e-01,\n",
              "        1.74604114e-02, -7.30028689e-01,  3.85564089e-01,  7.38579109e-02,\n",
              "       -5.44574074e-02,  2.78491050e-01, -5.20365357e-01,  2.34087348e-01,\n",
              "       -3.81479323e-01, -6.88205808e-02,  1.61007926e-01,  3.68799448e-01,\n",
              "        8.41858923e-01, -4.20245916e-01,  9.18819234e-02,  8.37017670e-02,\n",
              "       -3.00777666e-02,  8.52915719e-02,  2.60146081e-01, -3.19944590e-01,\n",
              "        2.93926209e-01,  3.93378228e-01,  4.83596951e-01, -4.58023489e-01,\n",
              "       -4.19259161e-01,  1.86483517e-01, -3.44224960e-01, -3.66644979e-01,\n",
              "       -3.11459690e-01, -5.38236313e-02,  1.49373531e-01,  5.29168189e-01,\n",
              "       -1.91751406e-01, -1.15635253e-01, -5.64483553e-02,  2.06099778e-01,\n",
              "        1.34294868e-01, -2.26314157e-01, -6.63047507e-02,  1.19542658e-01,\n",
              "       -1.21948877e-02, -1.56970888e-01, -1.90192088e-01,  1.38464063e-01,\n",
              "       -2.21104324e-01, -4.40817088e-01,  4.88236040e-01, -2.32119441e-01,\n",
              "       -9.54563320e-01,  1.60629526e-01,  5.82064129e-02, -1.90790683e-01,\n",
              "       -9.18238312e-02,  1.53605431e-01, -1.47697190e-02,  1.45433560e-01,\n",
              "       -4.01483208e-01, -1.49562642e-01, -2.57827453e-02, -2.47651041e-01,\n",
              "       -1.11028828e-01,  6.31714225e-01,  1.50557645e-02,  3.42476815e-02,\n",
              "        3.27138662e-01,  2.95237452e-01, -5.64751923e-01,  1.72929585e-01,\n",
              "        2.43234470e-01,  1.03276871e-01,  2.00290903e-02, -5.71414113e-01,\n",
              "        6.51847601e-01,  3.48923564e-01,  4.51909810e-01, -6.04974329e-01,\n",
              "       -4.85614181e-01, -4.27926600e-01, -2.21573919e-05,  7.07317829e-01,\n",
              "       -5.35285413e-01, -4.77035105e-01,  5.18273115e-01, -2.52808571e-01,\n",
              "        3.25124036e-03, -6.41550794e-02, -1.73677430e-02, -4.33730572e-01,\n",
              "        1.67348102e-01,  3.98033798e-01, -3.02660733e-01,  3.62665355e-01,\n",
              "        3.66713673e-01,  4.78746928e-02, -2.12043703e-01,  3.73666346e-01,\n",
              "        6.99329674e-02,  1.71972051e-01, -6.64744735e-01,  2.14124426e-01,\n",
              "        2.94099599e-01, -7.69467652e-01,  1.50758363e-02,  2.56381989e-01,\n",
              "        2.08643794e-01,  6.68318151e-03, -1.00124754e-01,  1.33072376e-01,\n",
              "       -9.86871198e-02,  2.74710417e-01, -5.69154099e-02, -1.25584722e-01,\n",
              "        7.37991333e-02, -1.08343989e-01,  7.47585250e-03,  3.36936206e-01,\n",
              "       -7.73561478e-01,  6.42022848e-01,  3.90431911e-01, -3.76996517e-01,\n",
              "        1.51560634e-01,  9.69111800e-01,  9.11481023e-01, -8.13824683e-02,\n",
              "       -1.95236087e-01, -5.94120286e-02,  8.82078335e-02, -1.76884234e-01,\n",
              "       -4.69746500e-01,  3.01109552e-01, -2.59901643e-01,  3.92412752e-01,\n",
              "       -5.12758374e-01, -6.66752905e-02, -2.83974022e-01,  5.97865820e-01,\n",
              "        8.41927677e-02, -2.19337959e-02, -1.07404783e-01,  8.07568152e-03,\n",
              "       -2.80854076e-01, -1.79564968e-01,  3.89197588e-01,  1.52475834e-01,\n",
              "       -5.40499873e-02, -1.33746877e-01, -1.19507536e-01,  1.59221545e-01,\n",
              "       -1.94792166e-01,  2.09635228e-01, -3.51974517e-01,  2.76347756e-01,\n",
              "        1.85992584e-01, -6.54922605e-01,  1.33606926e-01, -1.24270357e-01,\n",
              "        2.11550090e-02,  1.35255173e-01, -5.36351621e-01, -2.87048846e-01,\n",
              "        7.78131783e-01,  4.58934069e-01, -3.01510483e-01, -3.49822193e-01,\n",
              "        3.77804041e-01, -4.58727032e-01, -8.37465003e-02,  1.81347132e-01,\n",
              "        3.55815589e-01,  2.63936996e-01,  7.11375713e-01,  4.50247437e-01,\n",
              "        1.60743892e-01, -2.27487668e-01, -5.06291866e-01,  1.58439010e-01,\n",
              "       -1.49507925e-01,  4.62942086e-02, -5.79547405e-01, -5.48760109e-02,\n",
              "        2.96585858e-01,  7.15199113e-01, -6.27676725e-01, -1.46176696e-01,\n",
              "        1.91107675e-01, -4.60413963e-01, -4.32623811e-02,  1.20998789e-02,\n",
              "       -6.53645471e-02,  3.59326065e-01,  1.78218689e-02, -2.71465868e-01,\n",
              "       -6.42810464e-02,  1.21824943e-01,  1.55199111e-01,  6.70400500e-01,\n",
              "        6.69435620e-01, -8.38551968e-02,  8.09186623e-02, -2.55419891e-02,\n",
              "       -5.07680297e-01, -8.46803859e-02, -3.08684647e-01, -1.75641805e-01,\n",
              "        6.27608523e-02, -2.64231801e-01,  2.15090305e-01, -6.89629853e-01,\n",
              "        1.35672718e-01, -3.36556762e-01, -7.94957817e-01, -1.67218372e-01,\n",
              "        8.42298567e-02, -6.67588949e-01,  7.09614232e-02, -3.08052838e-01],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_vector(string):\n",
        "  len = 0\n",
        "  vector = np.zeros(300)\n",
        "  for word in string:\n",
        "    if type(word) != str:\n",
        "      continue\n",
        "    elif word not in w2v.wv.vocab:\n",
        "      continue\n",
        "    vector = vector + w2v.wv.get_vector(word)\n",
        "    len+=1\n",
        "\n",
        "\n",
        "  return vector/len"
      ],
      "metadata": {
        "id": "8FoL3NGWDQZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['text'] = df['text'].map(mean_vector)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrgsedrNGGON",
        "outputId": "9dcf2bec-f493-4f11-c1e3-9259a486245f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-28-b294a5bc2ff6>:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['text'] = df['text'].map(mean_vector)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vector_df = pd.DataFrame(df['text'].to_list(), columns=[f'feature_{i+1}' for i in range(300)])\n",
        "\n",
        "# Concatenate feature DataFrame with rubric column\n",
        "result  = pd.concat([df.reset_index().drop(\"index\", axis=1), vector_df], axis=1)"
      ],
      "metadata": {
        "id": "vXjsIS3qKZOX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 834
        },
        "id": "l3R5fEphLH-J",
        "outputId": "ecb5fed3-6205-4ca1-ec4e-7cebfc92a405"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                   text  rubric  feature_1  \\\n",
              "0     [-0.1419259850351539, 0.08997282496064293, -0....     Мир  -0.141926   \n",
              "1     [-0.17873677493087828, 0.03361459958204438, -0...  Россия  -0.178737   \n",
              "2     [-0.03762282231236221, 0.12408353854143234, -0...     Мир  -0.037623   \n",
              "3     [-0.07561444902149517, 0.032000629464164376, -...     Мир  -0.075614   \n",
              "4     [-0.05435901431609755, -0.040716265848554346, ...     Мир  -0.054359   \n",
              "...                                                 ...     ...        ...   \n",
              "1621  [-0.12852551789992317, 0.12083297538361987, -0...  Россия  -0.128526   \n",
              "1622  [-0.08483594141781647, 0.06638847651861687, 0....  Россия  -0.084836   \n",
              "1623  [-0.09809835494709122, -0.04048196648960058, -...  Россия  -0.098098   \n",
              "1624  [-0.10410822005594556, 0.10940082387467806, 0....     Мир  -0.104108   \n",
              "1625  [-0.059028609253194496, 0.14516073307944075, 0...     Мир  -0.059029   \n",
              "\n",
              "      feature_2  feature_3  feature_4  feature_5  feature_6  feature_7  \\\n",
              "0      0.089973  -0.182864   0.091670   0.175368   0.184784   0.028104   \n",
              "1      0.033615  -0.005492  -0.014667   0.077261   0.235475  -0.149112   \n",
              "2      0.124084  -0.031329   0.059092   0.144818   0.182589  -0.097897   \n",
              "3      0.032001  -0.051134   0.102946   0.143312   0.287640  -0.108037   \n",
              "4     -0.040716  -0.048610   0.067943   0.068528   0.167268  -0.005884   \n",
              "...         ...        ...        ...        ...        ...        ...   \n",
              "1621   0.120833  -0.089201  -0.108318   0.215013   0.168679   0.026345   \n",
              "1622   0.066388   0.006405  -0.009657   0.009611   0.169968  -0.084734   \n",
              "1623  -0.040482  -0.030405  -0.047726   0.076643   0.203821  -0.138404   \n",
              "1624   0.109401   0.016355   0.057165   0.187137   0.147629   0.005908   \n",
              "1625   0.145161   0.021705   0.059253  -0.022315   0.230222  -0.060726   \n",
              "\n",
              "      feature_8  ...  feature_291  feature_292  feature_293  feature_294  \\\n",
              "0     -0.156278  ...    -0.007197    -0.045465     0.029268     0.089884   \n",
              "1     -0.149998  ...    -0.062148    -0.155750     0.076475    -0.038653   \n",
              "2     -0.219109  ...    -0.075188    -0.130190     0.100777    -0.012934   \n",
              "3     -0.233706  ...    -0.035773    -0.107895    -0.032249    -0.078025   \n",
              "4     -0.204094  ...    -0.091118    -0.190087    -0.004587     0.118081   \n",
              "...         ...  ...          ...          ...          ...          ...   \n",
              "1621  -0.141776  ...    -0.129567    -0.032553     0.018072    -0.076139   \n",
              "1622  -0.096505  ...    -0.008029    -0.052033     0.015198    -0.115209   \n",
              "1623  -0.062946  ...     0.052257     0.064651    -0.017520    -0.035399   \n",
              "1624  -0.235896  ...     0.049636    -0.171586     0.021935    -0.053969   \n",
              "1625  -0.146255  ...     0.061037    -0.072902     0.001380     0.035335   \n",
              "\n",
              "      feature_295  feature_296  feature_297  feature_298  feature_299  \\\n",
              "0       -0.115882    -0.180595    -0.045483    -0.030673     0.007729   \n",
              "1       -0.199117    -0.201556    -0.074330     0.007207    -0.005383   \n",
              "2       -0.165758    -0.021185    -0.081217    -0.030957     0.069550   \n",
              "3       -0.021458    -0.123813     0.062838     0.061666    -0.012223   \n",
              "4       -0.074080    -0.044480    -0.028896     0.027143    -0.039566   \n",
              "...           ...          ...          ...          ...          ...   \n",
              "1621     0.038429    -0.123793     0.041669     0.005085    -0.119853   \n",
              "1622    -0.143036    -0.114123    -0.017190     0.002313     0.005189   \n",
              "1623    -0.023216    -0.211095     0.070182     0.098357    -0.038269   \n",
              "1624    -0.074176    -0.094896    -0.008423     0.003542    -0.009126   \n",
              "1625    -0.147219    -0.147616    -0.101263     0.093846     0.080758   \n",
              "\n",
              "      feature_300  \n",
              "0       -0.050785  \n",
              "1       -0.072535  \n",
              "2       -0.087013  \n",
              "3       -0.067549  \n",
              "4       -0.165588  \n",
              "...           ...  \n",
              "1621     0.067121  \n",
              "1622    -0.122192  \n",
              "1623    -0.127795  \n",
              "1624    -0.057312  \n",
              "1625    -0.124501  \n",
              "\n",
              "[1626 rows x 302 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c6914453-43b0-405a-9012-81382a9186cb\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>rubric</th>\n",
              "      <th>feature_1</th>\n",
              "      <th>feature_2</th>\n",
              "      <th>feature_3</th>\n",
              "      <th>feature_4</th>\n",
              "      <th>feature_5</th>\n",
              "      <th>feature_6</th>\n",
              "      <th>feature_7</th>\n",
              "      <th>feature_8</th>\n",
              "      <th>...</th>\n",
              "      <th>feature_291</th>\n",
              "      <th>feature_292</th>\n",
              "      <th>feature_293</th>\n",
              "      <th>feature_294</th>\n",
              "      <th>feature_295</th>\n",
              "      <th>feature_296</th>\n",
              "      <th>feature_297</th>\n",
              "      <th>feature_298</th>\n",
              "      <th>feature_299</th>\n",
              "      <th>feature_300</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[-0.1419259850351539, 0.08997282496064293, -0....</td>\n",
              "      <td>Мир</td>\n",
              "      <td>-0.141926</td>\n",
              "      <td>0.089973</td>\n",
              "      <td>-0.182864</td>\n",
              "      <td>0.091670</td>\n",
              "      <td>0.175368</td>\n",
              "      <td>0.184784</td>\n",
              "      <td>0.028104</td>\n",
              "      <td>-0.156278</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.007197</td>\n",
              "      <td>-0.045465</td>\n",
              "      <td>0.029268</td>\n",
              "      <td>0.089884</td>\n",
              "      <td>-0.115882</td>\n",
              "      <td>-0.180595</td>\n",
              "      <td>-0.045483</td>\n",
              "      <td>-0.030673</td>\n",
              "      <td>0.007729</td>\n",
              "      <td>-0.050785</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[-0.17873677493087828, 0.03361459958204438, -0...</td>\n",
              "      <td>Россия</td>\n",
              "      <td>-0.178737</td>\n",
              "      <td>0.033615</td>\n",
              "      <td>-0.005492</td>\n",
              "      <td>-0.014667</td>\n",
              "      <td>0.077261</td>\n",
              "      <td>0.235475</td>\n",
              "      <td>-0.149112</td>\n",
              "      <td>-0.149998</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.062148</td>\n",
              "      <td>-0.155750</td>\n",
              "      <td>0.076475</td>\n",
              "      <td>-0.038653</td>\n",
              "      <td>-0.199117</td>\n",
              "      <td>-0.201556</td>\n",
              "      <td>-0.074330</td>\n",
              "      <td>0.007207</td>\n",
              "      <td>-0.005383</td>\n",
              "      <td>-0.072535</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[-0.03762282231236221, 0.12408353854143234, -0...</td>\n",
              "      <td>Мир</td>\n",
              "      <td>-0.037623</td>\n",
              "      <td>0.124084</td>\n",
              "      <td>-0.031329</td>\n",
              "      <td>0.059092</td>\n",
              "      <td>0.144818</td>\n",
              "      <td>0.182589</td>\n",
              "      <td>-0.097897</td>\n",
              "      <td>-0.219109</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.075188</td>\n",
              "      <td>-0.130190</td>\n",
              "      <td>0.100777</td>\n",
              "      <td>-0.012934</td>\n",
              "      <td>-0.165758</td>\n",
              "      <td>-0.021185</td>\n",
              "      <td>-0.081217</td>\n",
              "      <td>-0.030957</td>\n",
              "      <td>0.069550</td>\n",
              "      <td>-0.087013</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[-0.07561444902149517, 0.032000629464164376, -...</td>\n",
              "      <td>Мир</td>\n",
              "      <td>-0.075614</td>\n",
              "      <td>0.032001</td>\n",
              "      <td>-0.051134</td>\n",
              "      <td>0.102946</td>\n",
              "      <td>0.143312</td>\n",
              "      <td>0.287640</td>\n",
              "      <td>-0.108037</td>\n",
              "      <td>-0.233706</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.035773</td>\n",
              "      <td>-0.107895</td>\n",
              "      <td>-0.032249</td>\n",
              "      <td>-0.078025</td>\n",
              "      <td>-0.021458</td>\n",
              "      <td>-0.123813</td>\n",
              "      <td>0.062838</td>\n",
              "      <td>0.061666</td>\n",
              "      <td>-0.012223</td>\n",
              "      <td>-0.067549</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[-0.05435901431609755, -0.040716265848554346, ...</td>\n",
              "      <td>Мир</td>\n",
              "      <td>-0.054359</td>\n",
              "      <td>-0.040716</td>\n",
              "      <td>-0.048610</td>\n",
              "      <td>0.067943</td>\n",
              "      <td>0.068528</td>\n",
              "      <td>0.167268</td>\n",
              "      <td>-0.005884</td>\n",
              "      <td>-0.204094</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.091118</td>\n",
              "      <td>-0.190087</td>\n",
              "      <td>-0.004587</td>\n",
              "      <td>0.118081</td>\n",
              "      <td>-0.074080</td>\n",
              "      <td>-0.044480</td>\n",
              "      <td>-0.028896</td>\n",
              "      <td>0.027143</td>\n",
              "      <td>-0.039566</td>\n",
              "      <td>-0.165588</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1621</th>\n",
              "      <td>[-0.12852551789992317, 0.12083297538361987, -0...</td>\n",
              "      <td>Россия</td>\n",
              "      <td>-0.128526</td>\n",
              "      <td>0.120833</td>\n",
              "      <td>-0.089201</td>\n",
              "      <td>-0.108318</td>\n",
              "      <td>0.215013</td>\n",
              "      <td>0.168679</td>\n",
              "      <td>0.026345</td>\n",
              "      <td>-0.141776</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.129567</td>\n",
              "      <td>-0.032553</td>\n",
              "      <td>0.018072</td>\n",
              "      <td>-0.076139</td>\n",
              "      <td>0.038429</td>\n",
              "      <td>-0.123793</td>\n",
              "      <td>0.041669</td>\n",
              "      <td>0.005085</td>\n",
              "      <td>-0.119853</td>\n",
              "      <td>0.067121</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1622</th>\n",
              "      <td>[-0.08483594141781647, 0.06638847651861687, 0....</td>\n",
              "      <td>Россия</td>\n",
              "      <td>-0.084836</td>\n",
              "      <td>0.066388</td>\n",
              "      <td>0.006405</td>\n",
              "      <td>-0.009657</td>\n",
              "      <td>0.009611</td>\n",
              "      <td>0.169968</td>\n",
              "      <td>-0.084734</td>\n",
              "      <td>-0.096505</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.008029</td>\n",
              "      <td>-0.052033</td>\n",
              "      <td>0.015198</td>\n",
              "      <td>-0.115209</td>\n",
              "      <td>-0.143036</td>\n",
              "      <td>-0.114123</td>\n",
              "      <td>-0.017190</td>\n",
              "      <td>0.002313</td>\n",
              "      <td>0.005189</td>\n",
              "      <td>-0.122192</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1623</th>\n",
              "      <td>[-0.09809835494709122, -0.04048196648960058, -...</td>\n",
              "      <td>Россия</td>\n",
              "      <td>-0.098098</td>\n",
              "      <td>-0.040482</td>\n",
              "      <td>-0.030405</td>\n",
              "      <td>-0.047726</td>\n",
              "      <td>0.076643</td>\n",
              "      <td>0.203821</td>\n",
              "      <td>-0.138404</td>\n",
              "      <td>-0.062946</td>\n",
              "      <td>...</td>\n",
              "      <td>0.052257</td>\n",
              "      <td>0.064651</td>\n",
              "      <td>-0.017520</td>\n",
              "      <td>-0.035399</td>\n",
              "      <td>-0.023216</td>\n",
              "      <td>-0.211095</td>\n",
              "      <td>0.070182</td>\n",
              "      <td>0.098357</td>\n",
              "      <td>-0.038269</td>\n",
              "      <td>-0.127795</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1624</th>\n",
              "      <td>[-0.10410822005594556, 0.10940082387467806, 0....</td>\n",
              "      <td>Мир</td>\n",
              "      <td>-0.104108</td>\n",
              "      <td>0.109401</td>\n",
              "      <td>0.016355</td>\n",
              "      <td>0.057165</td>\n",
              "      <td>0.187137</td>\n",
              "      <td>0.147629</td>\n",
              "      <td>0.005908</td>\n",
              "      <td>-0.235896</td>\n",
              "      <td>...</td>\n",
              "      <td>0.049636</td>\n",
              "      <td>-0.171586</td>\n",
              "      <td>0.021935</td>\n",
              "      <td>-0.053969</td>\n",
              "      <td>-0.074176</td>\n",
              "      <td>-0.094896</td>\n",
              "      <td>-0.008423</td>\n",
              "      <td>0.003542</td>\n",
              "      <td>-0.009126</td>\n",
              "      <td>-0.057312</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1625</th>\n",
              "      <td>[-0.059028609253194496, 0.14516073307944075, 0...</td>\n",
              "      <td>Мир</td>\n",
              "      <td>-0.059029</td>\n",
              "      <td>0.145161</td>\n",
              "      <td>0.021705</td>\n",
              "      <td>0.059253</td>\n",
              "      <td>-0.022315</td>\n",
              "      <td>0.230222</td>\n",
              "      <td>-0.060726</td>\n",
              "      <td>-0.146255</td>\n",
              "      <td>...</td>\n",
              "      <td>0.061037</td>\n",
              "      <td>-0.072902</td>\n",
              "      <td>0.001380</td>\n",
              "      <td>0.035335</td>\n",
              "      <td>-0.147219</td>\n",
              "      <td>-0.147616</td>\n",
              "      <td>-0.101263</td>\n",
              "      <td>0.093846</td>\n",
              "      <td>0.080758</td>\n",
              "      <td>-0.124501</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1626 rows × 302 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c6914453-43b0-405a-9012-81382a9186cb')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c6914453-43b0-405a-9012-81382a9186cb button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c6914453-43b0-405a-9012-81382a9186cb');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result.drop(['text'],axis =1, inplace = True)"
      ],
      "metadata": {
        "id": "M_LF32FWLsj3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "z8cVdnCbL1Ul",
        "outputId": "8f4879c4-d82f-4922-efaa-ca78e9b3f17c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      rubric  feature_1  feature_2  feature_3  feature_4  feature_5  \\\n",
              "0        Мир  -0.141926   0.089973  -0.182864   0.091670   0.175368   \n",
              "1     Россия  -0.178737   0.033615  -0.005492  -0.014667   0.077261   \n",
              "2        Мир  -0.037623   0.124084  -0.031329   0.059092   0.144818   \n",
              "3        Мир  -0.075614   0.032001  -0.051134   0.102946   0.143312   \n",
              "4        Мир  -0.054359  -0.040716  -0.048610   0.067943   0.068528   \n",
              "...      ...        ...        ...        ...        ...        ...   \n",
              "1621  Россия  -0.128526   0.120833  -0.089201  -0.108318   0.215013   \n",
              "1622  Россия  -0.084836   0.066388   0.006405  -0.009657   0.009611   \n",
              "1623  Россия  -0.098098  -0.040482  -0.030405  -0.047726   0.076643   \n",
              "1624     Мир  -0.104108   0.109401   0.016355   0.057165   0.187137   \n",
              "1625     Мир  -0.059029   0.145161   0.021705   0.059253  -0.022315   \n",
              "\n",
              "      feature_6  feature_7  feature_8  feature_9  ...  feature_291  \\\n",
              "0      0.184784   0.028104  -0.156278  -0.197947  ...    -0.007197   \n",
              "1      0.235475  -0.149112  -0.149998  -0.026083  ...    -0.062148   \n",
              "2      0.182589  -0.097897  -0.219109  -0.091652  ...    -0.075188   \n",
              "3      0.287640  -0.108037  -0.233706  -0.146325  ...    -0.035773   \n",
              "4      0.167268  -0.005884  -0.204094   0.084872  ...    -0.091118   \n",
              "...         ...        ...        ...        ...  ...          ...   \n",
              "1621   0.168679   0.026345  -0.141776  -0.174363  ...    -0.129567   \n",
              "1622   0.169968  -0.084734  -0.096505  -0.118263  ...    -0.008029   \n",
              "1623   0.203821  -0.138404  -0.062946  -0.137624  ...     0.052257   \n",
              "1624   0.147629   0.005908  -0.235896  -0.052837  ...     0.049636   \n",
              "1625   0.230222  -0.060726  -0.146255   0.026849  ...     0.061037   \n",
              "\n",
              "      feature_292  feature_293  feature_294  feature_295  feature_296  \\\n",
              "0       -0.045465     0.029268     0.089884    -0.115882    -0.180595   \n",
              "1       -0.155750     0.076475    -0.038653    -0.199117    -0.201556   \n",
              "2       -0.130190     0.100777    -0.012934    -0.165758    -0.021185   \n",
              "3       -0.107895    -0.032249    -0.078025    -0.021458    -0.123813   \n",
              "4       -0.190087    -0.004587     0.118081    -0.074080    -0.044480   \n",
              "...           ...          ...          ...          ...          ...   \n",
              "1621    -0.032553     0.018072    -0.076139     0.038429    -0.123793   \n",
              "1622    -0.052033     0.015198    -0.115209    -0.143036    -0.114123   \n",
              "1623     0.064651    -0.017520    -0.035399    -0.023216    -0.211095   \n",
              "1624    -0.171586     0.021935    -0.053969    -0.074176    -0.094896   \n",
              "1625    -0.072902     0.001380     0.035335    -0.147219    -0.147616   \n",
              "\n",
              "      feature_297  feature_298  feature_299  feature_300  \n",
              "0       -0.045483    -0.030673     0.007729    -0.050785  \n",
              "1       -0.074330     0.007207    -0.005383    -0.072535  \n",
              "2       -0.081217    -0.030957     0.069550    -0.087013  \n",
              "3        0.062838     0.061666    -0.012223    -0.067549  \n",
              "4       -0.028896     0.027143    -0.039566    -0.165588  \n",
              "...           ...          ...          ...          ...  \n",
              "1621     0.041669     0.005085    -0.119853     0.067121  \n",
              "1622    -0.017190     0.002313     0.005189    -0.122192  \n",
              "1623     0.070182     0.098357    -0.038269    -0.127795  \n",
              "1624    -0.008423     0.003542    -0.009126    -0.057312  \n",
              "1625    -0.101263     0.093846     0.080758    -0.124501  \n",
              "\n",
              "[1626 rows x 301 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7c5b8439-ca14-4ae0-acb3-192456b4d13e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>rubric</th>\n",
              "      <th>feature_1</th>\n",
              "      <th>feature_2</th>\n",
              "      <th>feature_3</th>\n",
              "      <th>feature_4</th>\n",
              "      <th>feature_5</th>\n",
              "      <th>feature_6</th>\n",
              "      <th>feature_7</th>\n",
              "      <th>feature_8</th>\n",
              "      <th>feature_9</th>\n",
              "      <th>...</th>\n",
              "      <th>feature_291</th>\n",
              "      <th>feature_292</th>\n",
              "      <th>feature_293</th>\n",
              "      <th>feature_294</th>\n",
              "      <th>feature_295</th>\n",
              "      <th>feature_296</th>\n",
              "      <th>feature_297</th>\n",
              "      <th>feature_298</th>\n",
              "      <th>feature_299</th>\n",
              "      <th>feature_300</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Мир</td>\n",
              "      <td>-0.141926</td>\n",
              "      <td>0.089973</td>\n",
              "      <td>-0.182864</td>\n",
              "      <td>0.091670</td>\n",
              "      <td>0.175368</td>\n",
              "      <td>0.184784</td>\n",
              "      <td>0.028104</td>\n",
              "      <td>-0.156278</td>\n",
              "      <td>-0.197947</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.007197</td>\n",
              "      <td>-0.045465</td>\n",
              "      <td>0.029268</td>\n",
              "      <td>0.089884</td>\n",
              "      <td>-0.115882</td>\n",
              "      <td>-0.180595</td>\n",
              "      <td>-0.045483</td>\n",
              "      <td>-0.030673</td>\n",
              "      <td>0.007729</td>\n",
              "      <td>-0.050785</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Россия</td>\n",
              "      <td>-0.178737</td>\n",
              "      <td>0.033615</td>\n",
              "      <td>-0.005492</td>\n",
              "      <td>-0.014667</td>\n",
              "      <td>0.077261</td>\n",
              "      <td>0.235475</td>\n",
              "      <td>-0.149112</td>\n",
              "      <td>-0.149998</td>\n",
              "      <td>-0.026083</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.062148</td>\n",
              "      <td>-0.155750</td>\n",
              "      <td>0.076475</td>\n",
              "      <td>-0.038653</td>\n",
              "      <td>-0.199117</td>\n",
              "      <td>-0.201556</td>\n",
              "      <td>-0.074330</td>\n",
              "      <td>0.007207</td>\n",
              "      <td>-0.005383</td>\n",
              "      <td>-0.072535</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Мир</td>\n",
              "      <td>-0.037623</td>\n",
              "      <td>0.124084</td>\n",
              "      <td>-0.031329</td>\n",
              "      <td>0.059092</td>\n",
              "      <td>0.144818</td>\n",
              "      <td>0.182589</td>\n",
              "      <td>-0.097897</td>\n",
              "      <td>-0.219109</td>\n",
              "      <td>-0.091652</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.075188</td>\n",
              "      <td>-0.130190</td>\n",
              "      <td>0.100777</td>\n",
              "      <td>-0.012934</td>\n",
              "      <td>-0.165758</td>\n",
              "      <td>-0.021185</td>\n",
              "      <td>-0.081217</td>\n",
              "      <td>-0.030957</td>\n",
              "      <td>0.069550</td>\n",
              "      <td>-0.087013</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Мир</td>\n",
              "      <td>-0.075614</td>\n",
              "      <td>0.032001</td>\n",
              "      <td>-0.051134</td>\n",
              "      <td>0.102946</td>\n",
              "      <td>0.143312</td>\n",
              "      <td>0.287640</td>\n",
              "      <td>-0.108037</td>\n",
              "      <td>-0.233706</td>\n",
              "      <td>-0.146325</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.035773</td>\n",
              "      <td>-0.107895</td>\n",
              "      <td>-0.032249</td>\n",
              "      <td>-0.078025</td>\n",
              "      <td>-0.021458</td>\n",
              "      <td>-0.123813</td>\n",
              "      <td>0.062838</td>\n",
              "      <td>0.061666</td>\n",
              "      <td>-0.012223</td>\n",
              "      <td>-0.067549</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Мир</td>\n",
              "      <td>-0.054359</td>\n",
              "      <td>-0.040716</td>\n",
              "      <td>-0.048610</td>\n",
              "      <td>0.067943</td>\n",
              "      <td>0.068528</td>\n",
              "      <td>0.167268</td>\n",
              "      <td>-0.005884</td>\n",
              "      <td>-0.204094</td>\n",
              "      <td>0.084872</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.091118</td>\n",
              "      <td>-0.190087</td>\n",
              "      <td>-0.004587</td>\n",
              "      <td>0.118081</td>\n",
              "      <td>-0.074080</td>\n",
              "      <td>-0.044480</td>\n",
              "      <td>-0.028896</td>\n",
              "      <td>0.027143</td>\n",
              "      <td>-0.039566</td>\n",
              "      <td>-0.165588</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1621</th>\n",
              "      <td>Россия</td>\n",
              "      <td>-0.128526</td>\n",
              "      <td>0.120833</td>\n",
              "      <td>-0.089201</td>\n",
              "      <td>-0.108318</td>\n",
              "      <td>0.215013</td>\n",
              "      <td>0.168679</td>\n",
              "      <td>0.026345</td>\n",
              "      <td>-0.141776</td>\n",
              "      <td>-0.174363</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.129567</td>\n",
              "      <td>-0.032553</td>\n",
              "      <td>0.018072</td>\n",
              "      <td>-0.076139</td>\n",
              "      <td>0.038429</td>\n",
              "      <td>-0.123793</td>\n",
              "      <td>0.041669</td>\n",
              "      <td>0.005085</td>\n",
              "      <td>-0.119853</td>\n",
              "      <td>0.067121</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1622</th>\n",
              "      <td>Россия</td>\n",
              "      <td>-0.084836</td>\n",
              "      <td>0.066388</td>\n",
              "      <td>0.006405</td>\n",
              "      <td>-0.009657</td>\n",
              "      <td>0.009611</td>\n",
              "      <td>0.169968</td>\n",
              "      <td>-0.084734</td>\n",
              "      <td>-0.096505</td>\n",
              "      <td>-0.118263</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.008029</td>\n",
              "      <td>-0.052033</td>\n",
              "      <td>0.015198</td>\n",
              "      <td>-0.115209</td>\n",
              "      <td>-0.143036</td>\n",
              "      <td>-0.114123</td>\n",
              "      <td>-0.017190</td>\n",
              "      <td>0.002313</td>\n",
              "      <td>0.005189</td>\n",
              "      <td>-0.122192</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1623</th>\n",
              "      <td>Россия</td>\n",
              "      <td>-0.098098</td>\n",
              "      <td>-0.040482</td>\n",
              "      <td>-0.030405</td>\n",
              "      <td>-0.047726</td>\n",
              "      <td>0.076643</td>\n",
              "      <td>0.203821</td>\n",
              "      <td>-0.138404</td>\n",
              "      <td>-0.062946</td>\n",
              "      <td>-0.137624</td>\n",
              "      <td>...</td>\n",
              "      <td>0.052257</td>\n",
              "      <td>0.064651</td>\n",
              "      <td>-0.017520</td>\n",
              "      <td>-0.035399</td>\n",
              "      <td>-0.023216</td>\n",
              "      <td>-0.211095</td>\n",
              "      <td>0.070182</td>\n",
              "      <td>0.098357</td>\n",
              "      <td>-0.038269</td>\n",
              "      <td>-0.127795</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1624</th>\n",
              "      <td>Мир</td>\n",
              "      <td>-0.104108</td>\n",
              "      <td>0.109401</td>\n",
              "      <td>0.016355</td>\n",
              "      <td>0.057165</td>\n",
              "      <td>0.187137</td>\n",
              "      <td>0.147629</td>\n",
              "      <td>0.005908</td>\n",
              "      <td>-0.235896</td>\n",
              "      <td>-0.052837</td>\n",
              "      <td>...</td>\n",
              "      <td>0.049636</td>\n",
              "      <td>-0.171586</td>\n",
              "      <td>0.021935</td>\n",
              "      <td>-0.053969</td>\n",
              "      <td>-0.074176</td>\n",
              "      <td>-0.094896</td>\n",
              "      <td>-0.008423</td>\n",
              "      <td>0.003542</td>\n",
              "      <td>-0.009126</td>\n",
              "      <td>-0.057312</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1625</th>\n",
              "      <td>Мир</td>\n",
              "      <td>-0.059029</td>\n",
              "      <td>0.145161</td>\n",
              "      <td>0.021705</td>\n",
              "      <td>0.059253</td>\n",
              "      <td>-0.022315</td>\n",
              "      <td>0.230222</td>\n",
              "      <td>-0.060726</td>\n",
              "      <td>-0.146255</td>\n",
              "      <td>0.026849</td>\n",
              "      <td>...</td>\n",
              "      <td>0.061037</td>\n",
              "      <td>-0.072902</td>\n",
              "      <td>0.001380</td>\n",
              "      <td>0.035335</td>\n",
              "      <td>-0.147219</td>\n",
              "      <td>-0.147616</td>\n",
              "      <td>-0.101263</td>\n",
              "      <td>0.093846</td>\n",
              "      <td>0.080758</td>\n",
              "      <td>-0.124501</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1626 rows × 301 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7c5b8439-ca14-4ae0-acb3-192456b4d13e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7c5b8439-ca14-4ae0-acb3-192456b4d13e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7c5b8439-ca14-4ae0-acb3-192456b4d13e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result['rubric'].replace({'Мир': 0,'Россия' : 1},inplace = True)\n",
        "\n"
      ],
      "metadata": {
        "id": "a3PTt89fNGpU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(result.drop(\"rubric\", axis=1), result[\"rubric\"], test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "clf = LogisticRegression(random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wVDZcBymN_BZ",
        "outputId": "bd200b90-b328-4ffe-f570-fb4d84018db8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.94\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/news.csv')\n",
        "df = df[df['rubric'].notna()]\n",
        "df = df.loc[(df['rubric'] == 'Мир') | (df['rubric'] == 'Россия')]\n",
        "df = df.drop(['source','title','publication_date','subrubric','tags'],axis =1 )"
      ],
      "metadata": {
        "id": "gy3uP3ESP6IJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_vector2(string):\n",
        "  len = 0\n",
        "  vector = np.zeros(300)\n",
        "  for word in string:\n",
        "    if type(word) != str:\n",
        "      continue\n",
        "    elif word not in model.wv.vocab:\n",
        "      print(word)\n",
        "      continue\n",
        "    vector = vector + model.wv.get_vector(word)\n",
        "    len+=1\n",
        "\n",
        "\n",
        "  return vector/len"
      ],
      "metadata": {
        "id": "vVQLIft1RVIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "# Load a pre-trained model (for example, fasttext in Russian)\n",
        "model = api.load(\"fasttext-wiki-news-subwords-300\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXeJkNNORrNH",
        "outputId": "6d608c1b-75b8-441c-b5fc-a735dd597d75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 958.5/958.4MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['text'] = df['text'].map(mean_vector2)\n",
        "vector_df = pd.DataFrame(df['text'].to_list(), columns=[f'feature_{i+1}' for i in range(300)])\n",
        "\n",
        "# Concatenate feature DataFrame with rubric column\n",
        "result  = pd.concat([df.reset_index().drop(\"index\", axis=1), vector_df], axis=1)\n",
        "result.drop(['text'],axis =1, inplace = True)\n",
        "result['rubric'].replace({'Мир': 0,'Россия' : 1},inplace = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HNN5Q3eDRbEH",
        "outputId": "37b9b13c-1826-4f47-bf7b-b908cadbb439"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-36-f12daee2bf56>:7: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
            "  elif word not in model.wv.vocab:\n",
            "<ipython-input-36-f12daee2bf56>:10: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
            "  vector = vector + model.wv.get_vector(word)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mВыходные данные были обрезаны до нескольких последних строк (5000).\u001b[0m\n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "HIDBKPgnReAL",
        "outputId": "de0dd707-8adf-487d-ee7e-04430eee1ca7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      rubric  feature_1  feature_2  feature_3  feature_4  feature_5  \\\n",
              "0          0   0.075685   0.004382   0.139067   0.028679  -0.048978   \n",
              "1          1   0.077364   0.005725   0.142494   0.025208  -0.044835   \n",
              "2          0   0.079992   0.004701   0.143073   0.028104  -0.045564   \n",
              "3          0   0.080086   0.001177   0.144464   0.035846  -0.049813   \n",
              "4          0   0.079105   0.004315   0.144591   0.032055  -0.046272   \n",
              "...      ...        ...        ...        ...        ...        ...   \n",
              "1621       1   0.076240   0.001596   0.142082   0.023175  -0.046439   \n",
              "1622       1   0.078729   0.002549   0.143345   0.032813  -0.041787   \n",
              "1623       1   0.079277   0.002999   0.142095   0.029161  -0.043922   \n",
              "1624       0   0.075249   0.002349   0.140390   0.018603  -0.042940   \n",
              "1625       0   0.077941   0.006978   0.135326   0.021687  -0.041501   \n",
              "\n",
              "      feature_6  feature_7  feature_8  feature_9  ...  feature_291  \\\n",
              "0      0.088437  -0.006073   0.203926  -0.015871  ...     0.035146   \n",
              "1      0.091299  -0.004981   0.206396  -0.014592  ...     0.038354   \n",
              "2      0.090876  -0.007949   0.198184  -0.015703  ...     0.035953   \n",
              "3      0.093705  -0.007734   0.200672  -0.018006  ...     0.037215   \n",
              "4      0.093622  -0.008312   0.203224  -0.017134  ...     0.037277   \n",
              "...         ...        ...        ...        ...  ...          ...   \n",
              "1621   0.088173  -0.004972   0.199041  -0.016052  ...     0.040935   \n",
              "1622   0.096082  -0.009845   0.194813  -0.017267  ...     0.040908   \n",
              "1623   0.091444  -0.006512   0.197385  -0.017127  ...     0.040127   \n",
              "1624   0.088804  -0.004711   0.199955  -0.015619  ...     0.040465   \n",
              "1625   0.085495  -0.005160   0.197602  -0.020280  ...     0.034202   \n",
              "\n",
              "      feature_292  feature_293  feature_294  feature_295  feature_296  \\\n",
              "0        0.061403    -0.225491    -0.132917     0.052802    -0.031589   \n",
              "1        0.058393    -0.219612    -0.134092     0.052511    -0.033842   \n",
              "2        0.061988    -0.222319    -0.135128     0.054385    -0.034483   \n",
              "3        0.062033    -0.228493    -0.137057     0.057074    -0.035946   \n",
              "4        0.061005    -0.227260    -0.136471     0.054719    -0.035788   \n",
              "...           ...          ...          ...          ...          ...   \n",
              "1621     0.059215    -0.213948    -0.134802     0.052328    -0.029442   \n",
              "1622     0.057791    -0.219311    -0.137586     0.055371    -0.035025   \n",
              "1623     0.056075    -0.216624    -0.135448     0.056304    -0.034808   \n",
              "1624     0.057635    -0.211296    -0.134257     0.052904    -0.028332   \n",
              "1625     0.051366    -0.207705    -0.130157     0.051832    -0.024798   \n",
              "\n",
              "      feature_297  feature_298  feature_299  feature_300  \n",
              "0        0.009965    -0.071924     0.045572     0.082618  \n",
              "1        0.007797    -0.074051     0.045947     0.087006  \n",
              "2        0.010809    -0.073933     0.047103     0.086959  \n",
              "3        0.010792    -0.072208     0.044556     0.085382  \n",
              "4        0.010975    -0.076733     0.046138     0.089062  \n",
              "...           ...          ...          ...          ...  \n",
              "1621     0.006607    -0.069459     0.043634     0.082685  \n",
              "1622     0.008323    -0.075881     0.047997     0.087897  \n",
              "1623     0.006841    -0.073808     0.046541     0.085945  \n",
              "1624     0.007894    -0.069327     0.044871     0.083315  \n",
              "1625     0.006797    -0.073134     0.049338     0.082354  \n",
              "\n",
              "[1626 rows x 301 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-22c67a34-8c95-4cb4-80e5-260565e77def\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>rubric</th>\n",
              "      <th>feature_1</th>\n",
              "      <th>feature_2</th>\n",
              "      <th>feature_3</th>\n",
              "      <th>feature_4</th>\n",
              "      <th>feature_5</th>\n",
              "      <th>feature_6</th>\n",
              "      <th>feature_7</th>\n",
              "      <th>feature_8</th>\n",
              "      <th>feature_9</th>\n",
              "      <th>...</th>\n",
              "      <th>feature_291</th>\n",
              "      <th>feature_292</th>\n",
              "      <th>feature_293</th>\n",
              "      <th>feature_294</th>\n",
              "      <th>feature_295</th>\n",
              "      <th>feature_296</th>\n",
              "      <th>feature_297</th>\n",
              "      <th>feature_298</th>\n",
              "      <th>feature_299</th>\n",
              "      <th>feature_300</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0.075685</td>\n",
              "      <td>0.004382</td>\n",
              "      <td>0.139067</td>\n",
              "      <td>0.028679</td>\n",
              "      <td>-0.048978</td>\n",
              "      <td>0.088437</td>\n",
              "      <td>-0.006073</td>\n",
              "      <td>0.203926</td>\n",
              "      <td>-0.015871</td>\n",
              "      <td>...</td>\n",
              "      <td>0.035146</td>\n",
              "      <td>0.061403</td>\n",
              "      <td>-0.225491</td>\n",
              "      <td>-0.132917</td>\n",
              "      <td>0.052802</td>\n",
              "      <td>-0.031589</td>\n",
              "      <td>0.009965</td>\n",
              "      <td>-0.071924</td>\n",
              "      <td>0.045572</td>\n",
              "      <td>0.082618</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0.077364</td>\n",
              "      <td>0.005725</td>\n",
              "      <td>0.142494</td>\n",
              "      <td>0.025208</td>\n",
              "      <td>-0.044835</td>\n",
              "      <td>0.091299</td>\n",
              "      <td>-0.004981</td>\n",
              "      <td>0.206396</td>\n",
              "      <td>-0.014592</td>\n",
              "      <td>...</td>\n",
              "      <td>0.038354</td>\n",
              "      <td>0.058393</td>\n",
              "      <td>-0.219612</td>\n",
              "      <td>-0.134092</td>\n",
              "      <td>0.052511</td>\n",
              "      <td>-0.033842</td>\n",
              "      <td>0.007797</td>\n",
              "      <td>-0.074051</td>\n",
              "      <td>0.045947</td>\n",
              "      <td>0.087006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0.079992</td>\n",
              "      <td>0.004701</td>\n",
              "      <td>0.143073</td>\n",
              "      <td>0.028104</td>\n",
              "      <td>-0.045564</td>\n",
              "      <td>0.090876</td>\n",
              "      <td>-0.007949</td>\n",
              "      <td>0.198184</td>\n",
              "      <td>-0.015703</td>\n",
              "      <td>...</td>\n",
              "      <td>0.035953</td>\n",
              "      <td>0.061988</td>\n",
              "      <td>-0.222319</td>\n",
              "      <td>-0.135128</td>\n",
              "      <td>0.054385</td>\n",
              "      <td>-0.034483</td>\n",
              "      <td>0.010809</td>\n",
              "      <td>-0.073933</td>\n",
              "      <td>0.047103</td>\n",
              "      <td>0.086959</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0.080086</td>\n",
              "      <td>0.001177</td>\n",
              "      <td>0.144464</td>\n",
              "      <td>0.035846</td>\n",
              "      <td>-0.049813</td>\n",
              "      <td>0.093705</td>\n",
              "      <td>-0.007734</td>\n",
              "      <td>0.200672</td>\n",
              "      <td>-0.018006</td>\n",
              "      <td>...</td>\n",
              "      <td>0.037215</td>\n",
              "      <td>0.062033</td>\n",
              "      <td>-0.228493</td>\n",
              "      <td>-0.137057</td>\n",
              "      <td>0.057074</td>\n",
              "      <td>-0.035946</td>\n",
              "      <td>0.010792</td>\n",
              "      <td>-0.072208</td>\n",
              "      <td>0.044556</td>\n",
              "      <td>0.085382</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0.079105</td>\n",
              "      <td>0.004315</td>\n",
              "      <td>0.144591</td>\n",
              "      <td>0.032055</td>\n",
              "      <td>-0.046272</td>\n",
              "      <td>0.093622</td>\n",
              "      <td>-0.008312</td>\n",
              "      <td>0.203224</td>\n",
              "      <td>-0.017134</td>\n",
              "      <td>...</td>\n",
              "      <td>0.037277</td>\n",
              "      <td>0.061005</td>\n",
              "      <td>-0.227260</td>\n",
              "      <td>-0.136471</td>\n",
              "      <td>0.054719</td>\n",
              "      <td>-0.035788</td>\n",
              "      <td>0.010975</td>\n",
              "      <td>-0.076733</td>\n",
              "      <td>0.046138</td>\n",
              "      <td>0.089062</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1621</th>\n",
              "      <td>1</td>\n",
              "      <td>0.076240</td>\n",
              "      <td>0.001596</td>\n",
              "      <td>0.142082</td>\n",
              "      <td>0.023175</td>\n",
              "      <td>-0.046439</td>\n",
              "      <td>0.088173</td>\n",
              "      <td>-0.004972</td>\n",
              "      <td>0.199041</td>\n",
              "      <td>-0.016052</td>\n",
              "      <td>...</td>\n",
              "      <td>0.040935</td>\n",
              "      <td>0.059215</td>\n",
              "      <td>-0.213948</td>\n",
              "      <td>-0.134802</td>\n",
              "      <td>0.052328</td>\n",
              "      <td>-0.029442</td>\n",
              "      <td>0.006607</td>\n",
              "      <td>-0.069459</td>\n",
              "      <td>0.043634</td>\n",
              "      <td>0.082685</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1622</th>\n",
              "      <td>1</td>\n",
              "      <td>0.078729</td>\n",
              "      <td>0.002549</td>\n",
              "      <td>0.143345</td>\n",
              "      <td>0.032813</td>\n",
              "      <td>-0.041787</td>\n",
              "      <td>0.096082</td>\n",
              "      <td>-0.009845</td>\n",
              "      <td>0.194813</td>\n",
              "      <td>-0.017267</td>\n",
              "      <td>...</td>\n",
              "      <td>0.040908</td>\n",
              "      <td>0.057791</td>\n",
              "      <td>-0.219311</td>\n",
              "      <td>-0.137586</td>\n",
              "      <td>0.055371</td>\n",
              "      <td>-0.035025</td>\n",
              "      <td>0.008323</td>\n",
              "      <td>-0.075881</td>\n",
              "      <td>0.047997</td>\n",
              "      <td>0.087897</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1623</th>\n",
              "      <td>1</td>\n",
              "      <td>0.079277</td>\n",
              "      <td>0.002999</td>\n",
              "      <td>0.142095</td>\n",
              "      <td>0.029161</td>\n",
              "      <td>-0.043922</td>\n",
              "      <td>0.091444</td>\n",
              "      <td>-0.006512</td>\n",
              "      <td>0.197385</td>\n",
              "      <td>-0.017127</td>\n",
              "      <td>...</td>\n",
              "      <td>0.040127</td>\n",
              "      <td>0.056075</td>\n",
              "      <td>-0.216624</td>\n",
              "      <td>-0.135448</td>\n",
              "      <td>0.056304</td>\n",
              "      <td>-0.034808</td>\n",
              "      <td>0.006841</td>\n",
              "      <td>-0.073808</td>\n",
              "      <td>0.046541</td>\n",
              "      <td>0.085945</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1624</th>\n",
              "      <td>0</td>\n",
              "      <td>0.075249</td>\n",
              "      <td>0.002349</td>\n",
              "      <td>0.140390</td>\n",
              "      <td>0.018603</td>\n",
              "      <td>-0.042940</td>\n",
              "      <td>0.088804</td>\n",
              "      <td>-0.004711</td>\n",
              "      <td>0.199955</td>\n",
              "      <td>-0.015619</td>\n",
              "      <td>...</td>\n",
              "      <td>0.040465</td>\n",
              "      <td>0.057635</td>\n",
              "      <td>-0.211296</td>\n",
              "      <td>-0.134257</td>\n",
              "      <td>0.052904</td>\n",
              "      <td>-0.028332</td>\n",
              "      <td>0.007894</td>\n",
              "      <td>-0.069327</td>\n",
              "      <td>0.044871</td>\n",
              "      <td>0.083315</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1625</th>\n",
              "      <td>0</td>\n",
              "      <td>0.077941</td>\n",
              "      <td>0.006978</td>\n",
              "      <td>0.135326</td>\n",
              "      <td>0.021687</td>\n",
              "      <td>-0.041501</td>\n",
              "      <td>0.085495</td>\n",
              "      <td>-0.005160</td>\n",
              "      <td>0.197602</td>\n",
              "      <td>-0.020280</td>\n",
              "      <td>...</td>\n",
              "      <td>0.034202</td>\n",
              "      <td>0.051366</td>\n",
              "      <td>-0.207705</td>\n",
              "      <td>-0.130157</td>\n",
              "      <td>0.051832</td>\n",
              "      <td>-0.024798</td>\n",
              "      <td>0.006797</td>\n",
              "      <td>-0.073134</td>\n",
              "      <td>0.049338</td>\n",
              "      <td>0.082354</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1626 rows × 301 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-22c67a34-8c95-4cb4-80e5-260565e77def')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-22c67a34-8c95-4cb4-80e5-260565e77def button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-22c67a34-8c95-4cb4-80e5-260565e77def');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(result.drop(\"rubric\", axis=1), result[\"rubric\"], test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "clf = LogisticRegression(random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "woqjm-30VTL2",
        "outputId": "e5d08bbb-9509-4cd1-9537-7e48cc648418"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "avtfBEVXIEZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Для \"своей\" модели брал вектор размерности 300, так как fasttext-wiki-news-subwords-300 имеет размерность 300. Можно увидеть, что на данном датасете победила моя модель. Однако предобученная модель очень хорошо справилась для незнакомого текста\n"
      ],
      "metadata": {
        "id": "noChOfDERQfn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Задание 4\n"
      ],
      "metadata": {
        "id": "SpkkYKB1JhC7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6E8mTRFL5fu",
        "outputId": "f460f664-c81c-4598-b485-bf3311746f98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m105.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.12.1-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.26.14)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.12.1 tokenizers-0.13.2 transformers-4.26.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from tabulate import tabulate\n",
        "from tqdm import trange\n",
        "import random"
      ],
      "metadata": {
        "id": "SdnBHveNLgdz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/IMDB data.csv')"
      ],
      "metadata": {
        "id": "hzPVnYTbLRF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['sentiment'].replace({'negative': 0,'positive' : 1},inplace = True)"
      ],
      "metadata": {
        "id": "AW1ds3tnLfb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "EyhIppRhLqY4",
        "outputId": "1cb311ed-57a5-4675-d11f-8d9aac1c4dd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                  review  sentiment\n",
              "0      One of the other reviewers has mentioned that ...          1\n",
              "1      A wonderful little production. <br /><br />The...          1\n",
              "2      I thought this was a wonderful way to spend ti...          1\n",
              "3      Basically there's a family where a little boy ...          0\n",
              "4      Petter Mattei's \"Love in the Time of Money\" is...          1\n",
              "...                                                  ...        ...\n",
              "49995  I thought this movie did a down right good job...          1\n",
              "49996  Bad plot, bad dialogue, bad acting, idiotic di...          0\n",
              "49997  I am a Catholic taught in parochial elementary...          0\n",
              "49998  I'm going to have to disagree with the previou...          0\n",
              "49999  No one expects the Star Trek movies to be high...          0\n",
              "\n",
              "[50000 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-dcfa7845-e963-46a8-9954-8d5873c62fd4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49995</th>\n",
              "      <td>I thought this movie did a down right good job...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49996</th>\n",
              "      <td>Bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49997</th>\n",
              "      <td>I am a Catholic taught in parochial elementary...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49998</th>\n",
              "      <td>I'm going to have to disagree with the previou...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49999</th>\n",
              "      <td>No one expects the Star Trek movies to be high...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>50000 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dcfa7845-e963-46a8-9954-8d5873c62fd4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-dcfa7845-e963-46a8-9954-8d5873c62fd4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-dcfa7845-e963-46a8-9954-8d5873c62fd4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = df.review.values\n",
        "labels = df.sentiment.values"
      ],
      "metadata": {
        "id": "ULDtsefnMKt5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case = True)"
      ],
      "metadata": {
        "id": "UUmdQ4qkMQOT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "c0bde21abcdc4adb99227af9741072fd",
            "593b2603dd5248ca88b395beac5e837a",
            "547d0d7934d945c0a1167af422fd6e0e",
            "e11d0767a3414fd692d4bd928f661667",
            "66f9b1da7fe34dc1923e9f40a6b7850a",
            "bd279957815f4e53b6f8c099070df063",
            "0b90d74c69104d888a87c6765e915025",
            "fa752624ae8a4e46b6ce8620dea2cc34",
            "8623a3d7c48d40238fec15b719d32db7",
            "f335176d8c8e4c9d846da08d0eaadf15",
            "8b94dea30c1c4d52b70f699f3da6e85a",
            "7bf1fe11f9ad4221a108e66b82a758e8",
            "01ba0a86c66541f8919bf26c39fb22a4",
            "5ca99313a9f6474d999252b4d62bcc57",
            "fb3bd3b936054be18e1206e4d6c61c6e",
            "d7bd152f3b8740b1aa0edf607c0c2547",
            "699c9c099d1f41f29f50f20ebfe29d36",
            "c5b9fc02b5724583a54a3b67248defeb",
            "7975baa9c0f547e3bd7e8b542b8e79f0",
            "a19dfab5736646888afdbaa78996ac0f",
            "e808a0ffaba04d90be253c34b0732943",
            "782f9887ef5046afaebf34d1f310466e",
            "7ae1f57eb1644050a614b751ae75b40b",
            "ff0a1a5d1c0d49229194d9ae17d942b3",
            "ce648a1430164f5b8ea7a0f120762e9f",
            "b0a975f9c3324ee58a5fb0f49c952149",
            "25e364323fda4ba39a3c3294db2ec7a7",
            "b0e22b5472b5439eb505adbf66d2bda5",
            "43b5992cb5d64439abd4583c1eec832d",
            "021006f74dce465eb3e9fc9382acc6e1",
            "79a545075bca41c097224c94cefcac2b",
            "4a1757a7e520402f86cdcb5d0ec3df1f",
            "86850656e51346669ae534a5729ccadf"
          ]
        },
        "outputId": "cad21937-bb51-4de9-9014-eb8c77cdabf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c0bde21abcdc4adb99227af9741072fd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7bf1fe11f9ad4221a108e66b82a758e8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7ae1f57eb1644050a614b751ae75b40b"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_id = []\n",
        "attention_masks = []\n",
        "\n",
        "def preprocessing(input_text, tokenizer):\n",
        "  '''\n",
        "  Returns <class transformers.tokenization_utils_base.BatchEncoding> with the following fields:\n",
        "    - input_ids: list of token ids\n",
        "    - token_type_ids: list of token type ids\n",
        "    - attention_mask: list of indices (0,1) specifying which tokens should considered by the model (return_attention_mask = True).\n",
        "  '''\n",
        "  return tokenizer.encode_plus(\n",
        "                        input_text,\n",
        "                        add_special_tokens = True,\n",
        "                        max_length = 32,\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,\n",
        "                        return_tensors = 'pt'\n",
        "                   )\n",
        "\n",
        "\n",
        "for sample in text:\n",
        "  encoding_dict = preprocessing(sample, tokenizer)\n",
        "  token_id.append(encoding_dict['input_ids']) \n",
        "  attention_masks.append(encoding_dict['attention_mask'])\n",
        "\n",
        "\n",
        "token_id = torch.cat(token_id, dim = 0)\n",
        "attention_masks = torch.cat(attention_masks, dim = 0)\n",
        "labels = torch.tensor(labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TjFQJb5OM3Me",
        "outputId": "ac945044-77f8-4012-f9b3-f2d9ad5a96ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2339: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_id[6]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qswscyy_M54u",
        "outputId": "40243c06-214d-4377-fed5-f4cbe772e449"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([  101,  1045,  2469,  2052,  2066,  2000,  2156,  1037, 15218,  1997,\n",
              "         1037,  2039,  6052,  2712, 17157,  2102,  2186,  2007,  1996,  6627,\n",
              "         2027,  2031,  2651,  2009,  2052,  3288,  2067,  1996,  4845,  8277,\n",
              "         1999,   102])"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def print_rand_sentence_encoding():\n",
        "  '''Displays tokens, token IDs and attention mask of a random text sample'''\n",
        "  index = random.randint(0, len(text) - 1)\n",
        "  tokens = tokenizer.tokenize(tokenizer.decode(token_id[index]))\n",
        "  token_ids = [i.numpy() for i in token_id[index]]\n",
        "  attention = [i.numpy() for i in attention_masks[index]]\n",
        "\n",
        "  table = np.array([tokens, token_ids, attention]).T\n",
        "  print(tabulate(table, \n",
        "                 headers = ['Tokens', 'Token IDs', 'Attention Mask'],\n",
        "                 tablefmt = 'fancy_grid'))\n",
        "\n",
        "print_rand_sentence_encoding()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2zJlqknNh7a",
        "outputId": "fc72d074-bc59-4748-abd9-ffebc2539bfd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "╒═════════════╤═════════════╤══════════════════╕\n",
            "│ Tokens      │   Token IDs │   Attention Mask │\n",
            "╞═════════════╪═════════════╪══════════════════╡\n",
            "│ [CLS]       │         101 │                1 │\n",
            "├─────────────┼─────────────┼──────────────────┤\n",
            "│ if          │        2065 │                1 │\n",
            "├─────────────┼─────────────┼──────────────────┤\n",
            "│ sicily      │       12071 │                1 │\n",
            "├─────────────┼─────────────┼──────────────────┤\n",
            "│ is          │        2003 │                1 │\n",
            "├─────────────┼─────────────┼──────────────────┤\n",
            "│ a           │        1037 │                1 │\n",
            "├─────────────┼─────────────┼──────────────────┤\n",
            "│ territory   │        3700 │                1 │\n",
            "├─────────────┼─────────────┼──────────────────┤\n",
            "│ of          │        1997 │                1 │\n",
            "├─────────────┼─────────────┼──────────────────┤\n",
            "│ the         │        1996 │                1 │\n",
            "├─────────────┼─────────────┼──────────────────┤\n",
            "│ baroque     │       10456 │                1 │\n",
            "├─────────────┼─────────────┼──────────────────┤\n",
            "│ ,           │        1010 │                1 │\n",
            "├─────────────┼─────────────┼──────────────────┤\n",
            "│ with        │        2007 │                1 │\n",
            "├─────────────┼─────────────┼──────────────────┤\n",
            "│ its         │        2049 │                1 │\n",
            "├─────────────┼─────────────┼──────────────────┤\n",
            "│ doubling    │       19383 │                1 │\n",
            "├─────────────┼─────────────┼──────────────────┤\n",
            "│ of          │        1997 │                1 │\n",
            "├─────────────┼─────────────┼──────────────────┤\n",
            "│ perspective │        7339 │                1 │\n",
            "├─────────────┼─────────────┼──────────────────┤\n",
            "│ ,           │        1010 │                1 │\n",
            "├─────────────┼─────────────┼──────────────────┤\n",
            "│ that        │        2008 │                1 │\n",
            "├─────────────┼─────────────┼──────────────────┤\n",
            "│ '           │        1005 │                1 │\n",
            "├─────────────┼─────────────┼──────────────────┤\n",
            "│ s           │        1055 │                1 │\n",
            "├─────────────┼─────────────┼──────────────────┤\n",
            "│ part        │        2112 │                1 │\n",
            "├─────────────┼─────────────┼──────────────────┤\n",
            "│ of          │        1997 │                1 │\n",
            "├─────────────┼─────────────┼──────────────────┤\n",
            "│ this        │        2023 │                1 │\n",
            "├─────────────┼─────────────┼──────────────────┤\n",
            "│ movie       │        3185 │                1 │\n",
            "├─────────────┼─────────────┼──────────────────┤\n",
            "│ '           │        1005 │                1 │\n",
            "├─────────────┼─────────────┼──────────────────┤\n",
            "│ s           │        1055 │                1 │\n",
            "├─────────────┼─────────────┼──────────────────┤\n",
            "│ challenge   │        4119 │                1 │\n",
            "├─────────────┼─────────────┼──────────────────┤\n",
            "│ to          │        2000 │                1 │\n",
            "├─────────────┼─────────────┼──────────────────┤\n",
            "│ realism     │       15650 │                1 │\n",
            "├─────────────┼─────────────┼──────────────────┤\n",
            "│ .           │        1012 │                1 │\n",
            "├─────────────┼─────────────┼──────────────────┤\n",
            "│ and         │        1998 │                1 │\n",
            "├─────────────┼─────────────┼──────────────────┤\n",
            "│ it          │        2009 │                1 │\n",
            "├─────────────┼─────────────┼──────────────────┤\n",
            "│ [SEP]       │         102 │                1 │\n",
            "╘═════════════╧═════════════╧══════════════════╛\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_ratio = 0.2\n",
        "# Recommended batch size: 16, 32. See: https://arxiv.org/pdf/1810.04805.pdf\n",
        "batch_size = 16\n",
        "\n",
        "# Indices of the train and validation splits stratified by labels\n",
        "train_idx, val_idx = train_test_split(\n",
        "    np.arange(len(labels)),\n",
        "    test_size = val_ratio,\n",
        "    shuffle = True,\n",
        "    stratify = labels)\n",
        "\n",
        "# Train and validation sets\n",
        "train_set = TensorDataset(token_id[train_idx], \n",
        "                          attention_masks[train_idx], \n",
        "                          labels[train_idx])\n",
        "\n",
        "val_set = TensorDataset(token_id[val_idx], \n",
        "                        attention_masks[val_idx], \n",
        "                        labels[val_idx])\n",
        "\n",
        "# Prepare DataLoader\n",
        "train_dataloader = DataLoader(\n",
        "            train_set,\n",
        "            sampler = RandomSampler(train_set),\n",
        "            batch_size = batch_size\n",
        "        )\n",
        "\n",
        "validation_dataloader = DataLoader(\n",
        "            val_set,\n",
        "            sampler = SequentialSampler(val_set),\n",
        "            batch_size = batch_size\n",
        "        )"
      ],
      "metadata": {
        "id": "jCmsvL-GNtYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def b_tp(preds, labels):\n",
        "  '''Returns True Positives (TP): count of correct predictions of actual class 1'''\n",
        "  return sum([preds == labels and preds == 1 for preds, labels in zip(preds, labels)])\n",
        "\n",
        "def b_fp(preds, labels):\n",
        "  '''Returns False Positives (FP): count of wrong predictions of actual class 1'''\n",
        "  return sum([preds != labels and preds == 1 for preds, labels in zip(preds, labels)])\n",
        "\n",
        "def b_tn(preds, labels):\n",
        "  '''Returns True Negatives (TN): count of correct predictions of actual class 0'''\n",
        "  return sum([preds == labels and preds == 0 for preds, labels in zip(preds, labels)])\n",
        "\n",
        "def b_fn(preds, labels):\n",
        "  '''Returns False Negatives (FN): count of wrong predictions of actual class 0'''\n",
        "  return sum([preds != labels and preds == 0 for preds, labels in zip(preds, labels)])\n",
        "\n",
        "def b_metrics(preds, labels):\n",
        "  '''\n",
        "  Returns the following metrics:\n",
        "    - accuracy    = (TP + TN) / N\n",
        "    - precision   = TP / (TP + FP)\n",
        "    - recall      = TP / (TP + FN)\n",
        "    - specificity = TN / (TN + FP)\n",
        "  '''\n",
        "  preds = np.argmax(preds, axis = 1).flatten()\n",
        "  labels = labels.flatten()\n",
        "  tp = b_tp(preds, labels)\n",
        "  tn = b_tn(preds, labels)\n",
        "  fp = b_fp(preds, labels)\n",
        "  fn = b_fn(preds, labels)\n",
        "  b_accuracy = (tp + tn) / len(labels)\n",
        "  b_precision = tp / (tp + fp) if (tp + fp) > 0 else 'nan'\n",
        "  b_recall = tp / (tp + fn) if (tp + fn) > 0 else 'nan'\n",
        "  b_specificity = tn / (tn + fp) if (tn + fp) > 0 else 'nan'\n",
        "  return b_accuracy, b_precision, b_recall, b_specificity"
      ],
      "metadata": {
        "id": "Ng15KvP3N2TU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    'bert-base-uncased',\n",
        "    num_labels = 2,\n",
        "    output_attentions = False,\n",
        "    output_hidden_states = False,\n",
        ")\n",
        "\n",
        "# Recommended learning rates (Adam): 5e-5, 3e-5, 2e-5. See: https://arxiv.org/pdf/1810.04805.pdf\n",
        "optimizer = torch.optim.AdamW(model.parameters(), \n",
        "                              lr = 5e-5,\n",
        "                              eps = 1e-08\n",
        "                              )\n",
        "\n",
        "# Run on GPU\n",
        "model.cuda()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "f31464ac755c4387b9a0e1d7c1f6de0d",
            "d26b06d195434ac08f04a0a1123e0a70",
            "ca64c90adc3d45e190098de6386312a9",
            "d9e4a71f8f0c4e29a828d8ecfbebdbf1",
            "76c7f22db9474a42865b98ee4bfa3f66",
            "648a12665c9242359d623f692564c267",
            "d4bf5deb520449ff9f9eeb4d596f1f9c",
            "17d68d2d762d40149a5d0d26bb966d59",
            "76ff9e6510ea43cfb350a3d0604fbcb8",
            "62ee0c47991b48d68f44852da1843f8f",
            "83e66e83add24508a0ee09ad23e052a7"
          ]
        },
        "id": "Ux7PDtXkN4gT",
        "outputId": "b393fa0d-e760-4b85-c68c-9df4f6affb06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f31464ac755c4387b9a0e1d7c1f6de0d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Первая модель\n"
      ],
      "metadata": {
        "id": "4twfhaeuS8hL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Recommended number of epochs: 2, 3, 4. See: https://arxiv.org/pdf/1810.04805.pdf\n",
        "epochs = 2\n",
        "\n",
        "for _ in trange(epochs, desc = 'Epoch'):\n",
        "    \n",
        "    # ========== Training ==========\n",
        "    \n",
        "    # Set model to training mode\n",
        "    model.train()\n",
        "    \n",
        "    # Tracking variables\n",
        "    tr_loss = 0\n",
        "    nb_tr_examples, nb_tr_steps = 0, 0\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        optimizer.zero_grad()\n",
        "        # Forward pass\n",
        "        train_output = model(b_input_ids, \n",
        "                             token_type_ids = None, \n",
        "                             attention_mask = b_input_mask, \n",
        "                             labels = b_labels)\n",
        "        # Backward pass\n",
        "        train_output.loss.backward()\n",
        "        optimizer.step()\n",
        "        # Update tracking variables\n",
        "        tr_loss += train_output.loss.item()\n",
        "        nb_tr_examples += b_input_ids.size(0)\n",
        "        nb_tr_steps += 1\n",
        "\n",
        "    # ========== Validation ==========\n",
        "\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    val_accuracy = []\n",
        "    val_precision = []\n",
        "    val_recall = []\n",
        "    val_specificity = []\n",
        "\n",
        "    for batch in validation_dataloader:\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        with torch.no_grad():\n",
        "          # Forward pass\n",
        "          eval_output = model(b_input_ids, \n",
        "                              token_type_ids = None, \n",
        "                              attention_mask = b_input_mask)\n",
        "        logits = eval_output.logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        # Calculate validation metrics\n",
        "        b_accuracy, b_precision, b_recall, b_specificity = b_metrics(logits, label_ids)\n",
        "        val_accuracy.append(b_accuracy)\n",
        "        # Update precision only when (tp + fp) !=0; ignore nan\n",
        "        if b_precision != 'nan': val_precision.append(b_precision)\n",
        "        # Update recall only when (tp + fn) !=0; ignore nan\n",
        "        if b_recall != 'nan': val_recall.append(b_recall)\n",
        "        # Update specificity only when (tn + fp) !=0; ignore nan\n",
        "        if b_specificity != 'nan': val_specificity.append(b_specificity)\n",
        "\n",
        "    print('\\n\\t - Train loss: {:.4f}'.format(tr_loss / nb_tr_steps))\n",
        "    print('\\t - Validation Accuracy: {:.4f}'.format(sum(val_accuracy)/len(val_accuracy)))\n",
        "    print('\\t - Validation Precision: {:.4f}'.format(sum(val_precision)/len(val_precision)) if len(val_precision)>0 else '\\t - Validation Precision: NaN')\n",
        "    print('\\t - Validation Recall: {:.4f}'.format(sum(val_recall)/len(val_recall)) if len(val_recall)>0 else '\\t - Validation Recall: NaN')\n",
        "    print('\\t - Validation Specificity: {:.4f}\\n'.format(sum(val_specificity)/len(val_specificity)) if len(val_specificity)>0 else '\\t - Validation Specificity: NaN')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29oPRs-3N6vQ",
        "outputId": "96d70eee-d2f7-41e3-b4ac-6e182d7172cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch:  50%|█████     | 1/2 [05:37<05:37, 337.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\t - Train loss: 0.4945\n",
            "\t - Validation Accuracy: 0.7816\n",
            "\t - Validation Precision: 0.7942\n",
            "\t - Validation Recall: 0.7598\n",
            "\t - Validation Specificity: 0.7969\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 100%|██████████| 2/2 [11:10<00:00, 335.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\t - Train loss: 0.3511\n",
            "\t - Validation Accuracy: 0.7833\n",
            "\t - Validation Precision: 0.7347\n",
            "\t - Validation Recall: 0.8805\n",
            "\t - Validation Specificity: 0.6807\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Можем заметить что у модели очень хороший recall"
      ],
      "metadata": {
        "id": "RRZFP_cVTD-x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(new_sentence):\n",
        "  # We need Token IDs and Attention Mask for inference on the new sentence\n",
        "  test_ids = []\n",
        "  test_attention_mask = []\n",
        "\n",
        "  # Apply the tokenizer\n",
        "  encoding = preprocessing(new_sentence, tokenizer)\n",
        "\n",
        "  # Extract IDs and Attention Mask\n",
        "  test_ids.append(encoding['input_ids'])\n",
        "  test_attention_mask.append(encoding['attention_mask'])\n",
        "  test_ids = torch.cat(test_ids, dim = 0)\n",
        "  test_attention_mask = torch.cat(test_attention_mask, dim = 0)\n",
        "\n",
        "  # Forward pass, calculate logit predictions\n",
        "  with torch.no_grad():\n",
        "    output = model(test_ids.to(device), token_type_ids = None, attention_mask = test_attention_mask.to(device))\n",
        "\n",
        "  prediction = 'Nice film bro ' if np.argmax(output.logits.cpu().numpy()).flatten().item() == 1 else 'Bad film Bro'\n",
        "\n",
        "  print('Input Sentence: ', new_sentence)\n",
        "  print('Predicted Class: ', prediction)"
      ],
      "metadata": {
        "id": "_hu7WkUvN9_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_sentence = 'Amazing film, i like it, breathtaking'\n",
        "predict(new_sentence)\n",
        "new_sentence = \"So boring, i cannot see that\"\n",
        "predict(new_sentence)\n",
        "new_sentence = \"Bruh, i am upset\"\n",
        "predict(new_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bPac8lmhN_7Y",
        "outputId": "850dc92d-a9eb-480d-e5ff-0cb59f0be96c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Sentence:  Amazing film, i like it, breathtaking\n",
            "Predicted Class:  Nice film bro \n",
            "Input Sentence:  So boring, i cannot see that\n",
            "Predicted Class:  Bad film Bro\n",
            "Input Sentence:  Bruh, i am upset\n",
            "Predicted Class:  Bad film Bro\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2339: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Вторая модель. увеличил lr и количество эпох"
      ],
      "metadata": {
        "id": "9i8Rtm0DTJd2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = BertForSequenceClassification.from_pretrained(\n",
        "    'bert-base-uncased',\n",
        "    num_labels = 2,\n",
        "    output_attentions = False,\n",
        "    output_hidden_states = False,\n",
        "    \n",
        ")\n",
        "\n",
        "# Recommended learning rates (Adam): 5e-5, 3e-5, 2e-5. See: https://arxiv.org/pdf/1810.04805.pdf\n",
        "optimizer = torch.optim.AdamW(model2.parameters(), \n",
        "                              lr = 3e-5,\n",
        "                              eps = 1e-08\n",
        "                              )\n",
        "\n",
        "# Run on GPU\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jp4kL8hwVLWo",
        "outputId": "dd091d6e-b457-46a9-9314-b852a17b2b5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model2.to(device)\n",
        "print(device)\n",
        "# Recommended number of epochs: 2, 3, 4. See: https://arxiv.org/pdf/1810.04805.pdf\n",
        "epochs = 3\n",
        "\n",
        "for _ in trange(epochs, desc = 'Epoch'):\n",
        "    \n",
        "    # ========== Training ==========\n",
        "    \n",
        "    # Set model2 to training mode\n",
        "    model2.train()\n",
        "    \n",
        "    # Tracking variables\n",
        "    tr_loss = 0\n",
        "    nb_tr_examples, nb_tr_steps = 0, 0\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        optimizer.zero_grad()\n",
        "        # Forward pass\n",
        "        train_output = model2(b_input_ids, \n",
        "                             token_type_ids = None, \n",
        "                             attention_mask = b_input_mask, \n",
        "                             labels = b_labels)\n",
        "        # Backward pass\n",
        "        train_output.loss.backward()\n",
        "        optimizer.step()\n",
        "        # Update tracking variables\n",
        "        tr_loss += train_output.loss.item()\n",
        "        nb_tr_examples += b_input_ids.size(0)\n",
        "        nb_tr_steps += 1\n",
        "\n",
        "    # ========== Validation ==========\n",
        "\n",
        "    # Set model2 to evaluation mode\n",
        "    model2.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    val_accuracy = []\n",
        "    val_precision = []\n",
        "    val_recall = []\n",
        "    val_specificity = []\n",
        "\n",
        "    for batch in validation_dataloader:\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        with torch.no_grad():\n",
        "          # Forward pass\n",
        "          eval_output = model2(b_input_ids, \n",
        "                              token_type_ids = None, \n",
        "                              attention_mask = b_input_mask)\n",
        "        logits = eval_output.logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        # Calculate validation metrics\n",
        "        b_accuracy, b_precision, b_recall, b_specificity = b_metrics(logits, label_ids)\n",
        "        val_accuracy.append(b_accuracy)\n",
        "        # Update precision only when (tp + fp) !=0; ignore nan\n",
        "        if b_precision != 'nan': val_precision.append(b_precision)\n",
        "        # Update recall only when (tp + fn) !=0; ignore nan\n",
        "        if b_recall != 'nan': val_recall.append(b_recall)\n",
        "        # Update specificity only when (tn + fp) !=0; ignore nan\n",
        "        if b_specificity != 'nan': val_specificity.append(b_specificity)\n",
        "\n",
        "    print('\\n\\t - Train loss: {:.4f}'.format(tr_loss / nb_tr_steps))\n",
        "    print('\\t - Validation Accuracy: {:.4f}'.format(sum(val_accuracy)/len(val_accuracy)))\n",
        "    print('\\t - Validation Precision: {:.4f}'.format(sum(val_precision)/len(val_precision)) if len(val_precision)>0 else '\\t - Validation Precision: NaN')\n",
        "    print('\\t - Validation Recall: {:.4f}'.format(sum(val_recall)/len(val_recall)) if len(val_recall)>0 else '\\t - Validation Recall: NaN')\n",
        "    print('\\t - Validation Specificity: {:.4f}\\n'.format(sum(val_specificity)/len(val_specificity)) if len(val_specificity)>0 else '\\t - Validation Specificity: NaN')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ev-UPZhdVRL-",
        "outputId": "8787b1d0-e166-42ac-c1ff-95c957e75f51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch:  33%|███▎      | 1/3 [05:34<11:08, 334.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\t - Train loss: 0.4817\n",
            "\t - Validation Accuracy: 0.7867\n",
            "\t - Validation Precision: 0.7542\n",
            "\t - Validation Recall: 0.8514\n",
            "\t - Validation Specificity: 0.7175\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:  67%|██████▋   | 2/3 [11:08<05:34, 334.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\t - Train loss: 0.3300\n",
            "\t - Validation Accuracy: 0.7952\n",
            "\t - Validation Precision: 0.7628\n",
            "\t - Validation Recall: 0.8560\n",
            "\t - Validation Specificity: 0.7305\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 100%|██████████| 3/3 [16:40<00:00, 333.63s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\t - Train loss: 0.1794\n",
            "\t - Validation Accuracy: 0.7928\n",
            "\t - Validation Precision: 0.8051\n",
            "\t - Validation Recall: 0.7714\n",
            "\t - Validation Specificity: 0.8079\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict2(new_sentence):\n",
        "  # We need Token IDs and Attention Mask for inference on the new sentence\n",
        "  test_ids = []\n",
        "  test_attention_mask = []\n",
        "\n",
        "  # Apply the tokenizer\n",
        "  encoding = preprocessing(new_sentence, tokenizer)\n",
        "\n",
        "  # Extract IDs and Attention Mask\n",
        "  test_ids.append(encoding['input_ids'])\n",
        "  test_attention_mask.append(encoding['attention_mask'])\n",
        "  test_ids = torch.cat(test_ids, dim = 0)\n",
        "  test_attention_mask = torch.cat(test_attention_mask, dim = 0)\n",
        "\n",
        "  # Forward pass, calculate logit predictions\n",
        "  with torch.no_grad():\n",
        "    output = model2(test_ids.to(device), token_type_ids = None, attention_mask = test_attention_mask.to(device))\n",
        "\n",
        "  prediction = 'Nice film bro ' if np.argmax(output.logits.cpu().numpy()).flatten().item() == 1 else 'Bad film Bro'\n",
        "\n",
        "  print('Input Sentence: ', new_sentence)\n",
        "  print('Predicted Class: ', prediction)"
      ],
      "metadata": {
        "id": "x6sG5it96Ejl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_sentence = 'I recently watched the latest blockbuster and I must say, it did not disappoint. The acting was superb and the special effects were truly impressive. The plot kept me on the edge of my seat and I found myself fully invested in the characters journeys. Overall, a great film that I would highly recommend.'\n",
        "predict2(new_sentence)\n",
        "new_sentence = \"I had high expectations for this movie, but unfortunately it fell short. The story was lackluster and the acting was mediocre at best. The special effects were impressive, but that alone couldn't make up for the lack of depth in the plot. Overall, I was disappointed and wouldn't recommend this film\"\n",
        "predict2(new_sentence)\n",
        "new_sentence = \"This movie was a pleasant surprise! I didn't have high expectations going in, but the story was engaging and the characters were well-developed. The acting was top-notch and the special effects were impressive. I found myself fully invested in the story and cared about the characters' fates. Overall, a great film that I would definitely recommend\"\n",
        "predict2(new_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZmOIXkB6F-u",
        "outputId": "db4fc6c8-7324-4ff1-f7cc-c128303a7f29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Sentence:  I recently watched the latest blockbuster and I must say, it did not disappoint. The acting was superb and the special effects were truly impressive. The plot kept me on the edge of my seat and I found myself fully invested in the characters journeys. Overall, a great film that I would highly recommend.\n",
            "Predicted Class:  Nice film bro \n",
            "Input Sentence:  I had high expectations for this movie, but unfortunately it fell short. The story was lackluster and the acting was mediocre at best. The special effects were impressive, but that alone couldn't make up for the lack of depth in the plot. Overall, I was disappointed and wouldn't recommend this film\n",
            "Predicted Class:  Bad film Bro\n",
            "Input Sentence:  This movie was a pleasant surprise! I didn't have high expectations going in, but the story was engaging and the characters were well-developed. The acting was top-notch and the special effects were impressive. I found myself fully invested in the story and cared about the characters' fates. Overall, a great film that I would definitely recommend\n",
            "Predicted Class:  Nice film bro \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Третья модель"
      ],
      "metadata": {
        "id": "OVb66ArZT9xc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model3 = BertForSequenceClassification.from_pretrained(\n",
        "    'bert-base-uncased',\n",
        "    num_labels = 2,\n",
        "    output_attentions = False,\n",
        "    output_hidden_states = False,\n",
        "    \n",
        ")\n",
        "\n",
        "# Recommended learning rates (Adam): 5e-5, 3e-5, 2e-5. See: https://arxiv.org/pdf/1810.04805.pdf\n",
        "optimizer = torch.optim.AdamW(model3.parameters(), \n",
        "                              lr = 1e-5,\n",
        "                              eps = 1e-08\n",
        "                              )\n",
        "\n",
        "# Run on GPU\n"
      ],
      "metadata": {
        "id": "YFhSizRfQREX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5476540-b4f3-4c3f-8dab-78dc6a322b86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model3.to(device)\n",
        "print(device)\n",
        "# Recommended number of epochs: 2, 3, 4. See: https://arxiv.org/pdf/1810.04805.pdf\n",
        "epochs = 5\n",
        "\n",
        "for _ in trange(epochs, desc = 'Epoch'):\n",
        "    \n",
        "    # ========== Training ==========\n",
        "    \n",
        "    # Set model3 to training mode\n",
        "    model3.train()\n",
        "    \n",
        "    # Tracking variables\n",
        "    tr_loss = 0\n",
        "    nb_tr_examples, nb_tr_steps = 0, 0\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        optimizer.zero_grad()\n",
        "        # Forward pass\n",
        "        train_output = model3(b_input_ids, \n",
        "                             token_type_ids = None, \n",
        "                             attention_mask = b_input_mask, \n",
        "                             labels = b_labels)\n",
        "        # Backward pass\n",
        "        train_output.loss.backward()\n",
        "        optimizer.step()\n",
        "        # Update tracking variables\n",
        "        tr_loss += train_output.loss.item()\n",
        "        nb_tr_examples += b_input_ids.size(0)\n",
        "        nb_tr_steps += 1\n",
        "\n",
        "    # ========== Validation ==========\n",
        "\n",
        "    # Set model2 to evaluation mode\n",
        "    model3.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    val_accuracy = []\n",
        "    val_precision = []\n",
        "    val_recall = []\n",
        "    val_specificity = []\n",
        "\n",
        "    for batch in validation_dataloader:\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        with torch.no_grad():\n",
        "          # Forward pass\n",
        "          eval_output = model2(b_input_ids, \n",
        "                              token_type_ids = None, \n",
        "                              attention_mask = b_input_mask)\n",
        "        logits = eval_output.logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        # Calculate validation metrics\n",
        "        b_accuracy, b_precision, b_recall, b_specificity = b_metrics(logits, label_ids)\n",
        "        val_accuracy.append(b_accuracy)\n",
        "        # Update precision only when (tp + fp) !=0; ignore nan\n",
        "        if b_precision != 'nan': val_precision.append(b_precision)\n",
        "        # Update recall only when (tp + fn) !=0; ignore nan\n",
        "        if b_recall != 'nan': val_recall.append(b_recall)\n",
        "        # Update specificity only when (tn + fp) !=0; ignore nan\n",
        "        if b_specificity != 'nan': val_specificity.append(b_specificity)\n",
        "\n",
        "    print('\\n\\t - Train loss: {:.4f}'.format(tr_loss / nb_tr_steps))\n",
        "    print('\\t - Validation Accuracy: {:.4f}'.format(sum(val_accuracy)/len(val_accuracy)))\n",
        "    print('\\t - Validation Precision: {:.4f}'.format(sum(val_precision)/len(val_precision)) if len(val_precision)>0 else '\\t - Validation Precision: NaN')\n",
        "    print('\\t - Validation Recall: {:.4f}'.format(sum(val_recall)/len(val_recall)) if len(val_recall)>0 else '\\t - Validation Recall: NaN')\n",
        "    print('\\t - Validation Specificity: {:.4f}\\n'.format(sum(val_specificity)/len(val_specificity)) if len(val_specificity)>0 else '\\t - Validation Specificity: NaN')"
      ],
      "metadata": {
        "id": "R7gxU44OQWkh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd84371a-48c5-452e-a892-c53e87afe078"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch:  20%|██        | 1/5 [05:32<22:08, 332.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\t - Train loss: 0.4837\n",
            "\t - Validation Accuracy: 0.7928\n",
            "\t - Validation Precision: 0.8051\n",
            "\t - Validation Recall: 0.7714\n",
            "\t - Validation Specificity: 0.8079\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:  40%|████      | 2/5 [11:04<16:37, 332.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\t - Train loss: 0.3640\n",
            "\t - Validation Accuracy: 0.7928\n",
            "\t - Validation Precision: 0.8051\n",
            "\t - Validation Recall: 0.7714\n",
            "\t - Validation Specificity: 0.8079\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:  60%|██████    | 3/5 [16:37<11:04, 332.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\t - Train loss: 0.2473\n",
            "\t - Validation Accuracy: 0.7928\n",
            "\t - Validation Precision: 0.8051\n",
            "\t - Validation Recall: 0.7714\n",
            "\t - Validation Specificity: 0.8079\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:  80%|████████  | 4/5 [22:10<05:32, 332.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\t - Train loss: 0.1419\n",
            "\t - Validation Accuracy: 0.7928\n",
            "\t - Validation Precision: 0.8051\n",
            "\t - Validation Recall: 0.7714\n",
            "\t - Validation Specificity: 0.8079\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 100%|██████████| 5/5 [27:42<00:00, 332.58s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\t - Train loss: 0.0840\n",
            "\t - Validation Accuracy: 0.7928\n",
            "\t - Validation Precision: 0.8051\n",
            "\t - Validation Recall: 0.7714\n",
            "\t - Validation Specificity: 0.8079\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# У этой модели почему-то не менялись параметры кроме train loss, не смог понять почему так, но вероятно она переобучилась"
      ],
      "metadata": {
        "id": "dhrqmMEUUFDi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict3(new_sentence):\n",
        "  # We need Token IDs and Attention Mask for inference on the new sentence\n",
        "  test_ids = []\n",
        "  test_attention_mask = []\n",
        "\n",
        "  # Apply the tokenizer\n",
        "  encoding = preprocessing(new_sentence, tokenizer)\n",
        "\n",
        "  # Extract IDs and Attention Mask\n",
        "  test_ids.append(encoding['input_ids'])\n",
        "  test_attention_mask.append(encoding['attention_mask'])\n",
        "  test_ids = torch.cat(test_ids, dim = 0)\n",
        "  test_attention_mask = torch.cat(test_attention_mask, dim = 0)\n",
        "\n",
        "  # Forward pass, calculate logit predictions\n",
        "  with torch.no_grad():\n",
        "    output = model3(test_ids.to(device), token_type_ids = None, attention_mask = test_attention_mask.to(device))\n",
        "\n",
        "  prediction = 'Nice film bro ' if np.argmax(output.logits.cpu().numpy()).flatten().item() == 1 else 'Bad film Bro'\n",
        "\n",
        "  print('Input Sentence: ', new_sentence)\n",
        "  print('Predicted Class: ', prediction)"
      ],
      "metadata": {
        "id": "QpWSMOuRQggj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_sentence = 'I recently watched the latest blockbuster and I must say, it did not disappoint. The acting was superb and the special effects were truly impressive. The plot kept me on the edge of my seat and I found myself fully invested in the characters journeys. Overall, a great film that I would highly recommend.'\n",
        "predict3(new_sentence)\n",
        "new_sentence = \"I had high expectations for this movie, but unfortunately it fell short. The story was lackluster and the acting was mediocre at best. The special effects were impressive, but that alone couldn't make up for the lack of depth in the plot. Overall, I was disappointed and wouldn't recommend this film\"\n",
        "predict3(new_sentence)\n",
        "new_sentence = \"This movie was a pleasant surprise! I didn't have high expectations going in, but the story was engaging and the characters were well-developed. The acting was top-notch and the special effects were impressive. I found myself fully invested in the story and cared about the characters' fates. Overall, a great film that I would definitely recommend\"\n",
        "predict3(new_sentence)"
      ],
      "metadata": {
        "id": "iu1vsFcRQjqO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ccd8568-40ed-4429-e731-74f5c64b619a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Sentence:  I recently watched the latest blockbuster and I must say, it did not disappoint. The acting was superb and the special effects were truly impressive. The plot kept me on the edge of my seat and I found myself fully invested in the characters journeys. Overall, a great film that I would highly recommend.\n",
            "Predicted Class:  Nice film bro \n",
            "Input Sentence:  I had high expectations for this movie, but unfortunately it fell short. The story was lackluster and the acting was mediocre at best. The special effects were impressive, but that alone couldn't make up for the lack of depth in the plot. Overall, I was disappointed and wouldn't recommend this film\n",
            "Predicted Class:  Bad film Bro\n",
            "Input Sentence:  This movie was a pleasant surprise! I didn't have high expectations going in, but the story was engaging and the characters were well-developed. The acting was top-notch and the special effects were impressive. I found myself fully invested in the story and cared about the characters' fates. Overall, a great film that I would definitely recommend\n",
            "Predicted Class:  Nice film bro \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Вывод : Настройка параметров Bert, довольно сложная и вычислительно долгая процедура. Все модели получились +- одинаковые по качеству"
      ],
      "metadata": {
        "id": "T287Ynl0UVEb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "GAqCoGg-UXKH"
      }
    }
  ]
}